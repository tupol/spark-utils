[info] Loading settings for project global-plugins from sonatype.sbt,gpg.sbt ...
[info] Loading global plugins from /Users/olivertupran/.sbt/1.0/plugins
[info] Loading settings for project spark-utils-build from plugins.sbt ...
[info] Loading project definition from /Users/olivertupran/work/tupol/spark-utils/project
[info] Loading settings for project spark-utils from version.sbt,build.sbt ...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[success] Total time: 0 s, completed 5 Apr 2024, 08:01:40
[info] Forcing Scala version to 2.12.19 on all projects.
[info] Reapplying settings...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[info] Compiling 15 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/classes ...
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[info] Done compiling.
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes ...
[info] Compiling 20 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.12/classes ...
[info] Done compiling.
[info] Compiling 7 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.12/classes ...
[info] Done compiling.
[info] Compiling 27 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.12/test-classes ...
[info] + makeNameAvroCompliant.makeNameAvroCompliant should not change the name if the name is Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should change the name if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce shorter names if the name is not Avro compliant and replaceWith, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce same size names if the name is not Avro compliant and replaceWith is a single char, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should prepend the specified prefix if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should append the specified suffix if the name is not Avro compliant: OK, passed 100 tests.
[info] ConfigSpec:
[info] FuzzyTypesafeConfigBuilder.getConfiguration
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.whoami="app.param"
MockApp.param="param"
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-f15a5eea-ee5d-44bf-a344-66a9931d8c60/app.conf is available.
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-f15a5eea-ee5d-44bf-a344-66a9931d8c60/app.conf'
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
2024-04-05 08:02:34 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-05 08:02:34 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app params then defaults to the Spark app.conf file, then to the app.conf in the classpath and then to reference.conf
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockFun.param="param"
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-20408d3c-9c2a-411c-89ad-3c2f723cfce9/fun.conf is available.
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-20408d3c-9c2a-411c-89ad-3c2f723cfce9/fun.conf'
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/fun.conf is not available.
2024-04-05 08:02:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'fun.conf'
2024-04-05 08:02:34 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-05 08:02:34 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app.conf then defaults to reference.conf
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param="param"
my.var="MYVAR"
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-30f8154d-f256-4eaf-92d5-003949a595ec/app.conf is available.
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-30f8154d-f256-4eaf-92d5-003949a595ec/app.conf'
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
[info] - should perform variable substitution
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param=param
MockApp.my.var=MYVAR
MockApp.substitute.my-other-var=MY_OTHER_VAR
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-b704a6c6-a40e-4a8a-9de6-26dfa5596ec5/MockApp/app2.conf is not available.
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/MockApp/app2.conf is not available.
2024-04-05 08:02:35 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'MockApp/app2.conf'
[info] - should perform variable substitution and addition
[info] SimpleTypesafeConfigBuilder
[info] - should loads first the app params then defaults to app.conf file, then to the app.conf in the classpath and then to reference.conf
[info] - should defaults to reference.conf
[info] - should perform variable substitution
[info] SparkFunSpec:
2024-04-05 08:02:36 INFO  SparkFunSpec$MockFun$:62 - Running MockFun
2024-04-05 08:02:36 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:36 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-8e88b401-d6bd-4ae3-b3ea-874073b9b3f3/application.conf is not available.
2024-04-05 08:02:36 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:36 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:36 DEBUG SparkFunSpec$MockFun$:97 - MockFun: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_fun",
    "whoami" : "./src/test/resources/reference.conf"
}

2024-04-05 08:02:37 INFO  SparkFunSpec$MockFun$:72 - MockFun: Job successfully completed.
[info] - SparkFun.main successfully completes
2024-04-05 08:02:37 INFO  SparkFunSpec$MockFunNoConfig:62 - Running MockFunNoConfig
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-ab0c4cce-a214-43c6-893b-1b3a040dfdf1/application.conf is not available.
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:37 ERROR SparkFunSpec$MockFunNoConfig:101 - MockFunNoConfig: Failed to load application configuration from the MockFunNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunNoConfig'
2024-04-05 08:02:37 INFO  SparkFunSpec$MockFunNoConfig:72 - MockFunNoConfig: Job successfully completed.
[info] - SparkFun.main successfully completes with no configuration expected
2024-04-05 08:02:37 INFO  SparkFunSpec$MockFunFailure:62 - Running MockFunFailure
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-52ac756c-4f65-4536-9e7e-7ed67a8d0c95/application.conf is not available.
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:37 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:37 ERROR SparkFunSpec$MockFunFailure:101 - MockFunFailure: Failed to load application configuration from the MockFunFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunFailure'
2024-04-05 08:02:37 ERROR SparkFunSpec$MockFunFailure:73 - MockFunFailure: Job failed.
org.tupol.spark.SparkFunSpec$MockApException
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:54)
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:53)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkFun.main(SparkFun.scala:20)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$6(SparkFunSpec.scala:31)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$5(SparkFunSpec.scala:31)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkFunSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSpec.scala:11)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkFunSpec.runTest(SparkFunSpec.scala:11)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkFun.main fails gracefully if SparkFun.run fails
[info] - SparkFun.appName gets the simple class name
[info] MakeNameAvroCompliantSpec:
[info] - makeNameAvroCompliant removes the non-compliant chars
[info] - makeNameAvroCompliant replaces the first non-compliant char
[info] - makeNameAvroCompliant does not replace the first non-compliant char if a prefix is specified
[info] - makeNameAvroCompliant does not work with empty strings
[info] - makeNameAvroCompliant reports non-compliant prefix
[info] - makeNameAvroCompliant reports non-compliant replaceWith
[info] - makeNameAvroCompliant reports non-compliant replaceWith first char if prefix is not specified
[info] - makeNameAvroCompliant works with non-compliant replaceWith first char if prefix is specified
[info] - makeNameAvroCompliant reports non-compliant suffix
[info] Done compiling.
[info] - makeNameAvroCompliant successfully cleans the schema
[info] KeyValueDatasetOpsTest:
[info] mapValues
[info] - should map only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] flatMapValues
[info] - should flatMap only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] SchemaOpsSpec:
[info] - mapFields & checkAllFields
[info] - checkAllFields should fail inside array
[info] - checkAllFields should fail inside map
[info] - checkAnyFields plainly
[info] - checkAnyFields inside arrays
[info] - checkAnyFields inside maps
[info] DataFrameOpsSpec:
[info] - flattenFields on a flat DataFrame should produce no changes.
[info] - flattenFields on a structured DataFrame should flatten it.
[info] UtilsSpec:
2024-04-05 08:02:47 ERROR package:93 - Failed to load text resource from ''.
java.lang.IllegalArgumentException: Cannot load a text resource from an empty path.
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:47)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.app.UtilsSpec.$anonfun$new$1(UtilsSpec.scala:14)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - fuzzyLoadTextResourceFile fails while trying to load an empty path
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from local file '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URI '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URL '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from classpath '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-05 08:02:47 ERROR package:93 - Failed to load text resource from '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
java.lang.IllegalArgumentException: Unable to find '/unknown/path/leading/to/unknown/file/s1n.ci7y' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.app.UtilsSpec.$anonfun$new$2(UtilsSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/utils/sample-text.resource'.
[info] - fuzzyLoadTextResourceFile fails while trying to load a path that does not exist anywhere
2024-04-05 08:02:47 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from local file 'utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URI 'utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URL 'utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from classpath 'utils/sample-text.resource'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from a local path
2024-04-05 08:02:47 DEBUG package:56 - Successfully loaded text resource from classpath 'utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from local file 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Successfully loaded resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from local file 'http://info.cern.ch'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URI 'http://info.cern.ch'.
2024-04-05 08:02:47 DEBUG package:56 - Try loading text resource from URL 'http://info.cern.ch'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from the class path
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URI
2024-04-05 08:02:47 DEBUG package:56 - Successfully loaded resource from URL 'http://info.cern.ch'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URL
[info] MapOpsRowOpsSpec:
[info] - Converting a map to a row and the conversion back from a row to a map
[info] DatasetOpsTest:
[info] withTupledColumn
[info] - should return a tuple of input and column with a simple dataset of 1 value
[info] - should return a tuple of input and column with a simple dataset of 2 values
[info] - should return a tuple of input and column with a simple dataset of nested values
[info] - should return an empty dataset for an empty dataset
[info] - should fail if the specified column type does not match the actual column type
[info] SparkAppSpec:
2024-04-05 08:02:49 INFO  SparkAppSpec$MockApp$:62 - Running MockApp
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-302b6c57-b2bc-4fbf-a433-e651ad7f2852/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:49 DEBUG SparkAppSpec$MockApp$:97 - MockApp: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_app",
    "whoami" : "./src/test/resources/reference.conf"
}

2024-04-05 08:02:49 INFO  SparkAppSpec$MockApp$:72 - MockApp: Job successfully completed.
[info] - SparkApp.main successfully completes
[info] Done compiling.
2024-04-05 08:02:49 INFO  SparkAppSpec$MockAppNoConfig:62 - Running MockAppNoConfig
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-99720daf-ef19-4f24-8bda-2fa7d2298a72/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:49 ERROR SparkAppSpec$MockAppNoConfig:101 - MockAppNoConfig: Failed to load application configuration from the MockAppNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppNoConfig'
2024-04-05 08:02:49 INFO  SparkAppSpec$MockAppNoConfig:72 - MockAppNoConfig: Job successfully completed.
[info] - SparkApp.main successfully completes with no configuration expected
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.12/test-classes ...
2024-04-05 08:02:49 INFO  SparkAppSpec$MockAppFailure:62 - Running MockAppFailure
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-8a7f402a-a25b-4892-9ccf-d44a9caa45d7/userFiles-d3598225-c37b-487e-892b-fe6a164896f8/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-05 08:02:49 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-05 08:02:49 ERROR SparkAppSpec$MockAppFailure:101 - MockAppFailure: Failed to load application configuration from the MockAppFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppFailure'
2024-04-05 08:02:49 ERROR SparkAppSpec$MockAppFailure:73 - MockAppFailure: Job failed.
org.tupol.spark.SparkAppSpec$MockApException
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:56)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.main(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$6(SparkAppSpec.scala:33)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$5(SparkAppSpec.scala:33)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkAppSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkAppSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkAppSpec.runTest(SparkAppSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkApp.main fails gracefully if SparkApp.run fails
[info] - SparkApp.appName gets the simple class name
[info] GenericDataSourceSpec:
2024-04-05 08:02:54 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-05 08:02:54 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:02:54 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$2(GenericDataSourceSpec.scala:23)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way;
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:784)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:767)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:590)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:415)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$2(GenericDataSource.scala:65)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:65)
	... 47 more
2024-04-05 08:02:54 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-05 08:02:54 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:02:55 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$3(GenericDataSourceSpec.scala:25)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:25)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way;
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:784)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:767)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:590)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:415)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$2(GenericDataSource.scala:65)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:65)
	... 47 more
2024-04-05 08:02:55 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-05 08:02:55 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
[info] - Loading the data fails if the file does not exist
2024-04-05 08:02:56 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
2024-04-05 08:02:59 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-05 08:02:59 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-05 08:02:59 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-05 08:02:59 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:02:59 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
[info] Done compiling.
[info] - The number of records in the file provided and the schema must match
2024-04-05 08:03:07 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-05 08:03:07 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-05 08:03:08 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-05 08:03:08 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-05 08:03:08 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2024-04-05 08:03:08 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-05 08:03:08 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-05 08:03:08 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
[info] - The number of records in the file provided and the other schema must match
[info] addOptions
[info] - should leave the empty options unchanged
[info] - should leave the options unchanged
[info] - should add extra options
[info] - should override options
[info] withSchema
[info] - should set a new schema
[info] - should change an existing schema
[info] - should remove an existing schema
[info] OrcFileDataSourceSpec:
2024-04-05 08:03:13 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-05 08:03:13 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-05 08:03:13 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
2024-04-05 08:03:13 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-05 08:03:13 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-05 08:03:13 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-05 08:03:13 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-05 08:03:13 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
[info] - The number of records in the file provided and the schema must match
[info] StreamingSinkConfigurationSpec:
[info] addOptions
[info] - should overwrite existing options for GenericStreamDataSinkConfiguration
[info] - should overwrite existing options for FileStreamDataSinkConfiguration
[info] - should overwrite existing options for KafkaStreamDataSinkConfiguration
[info] FileDataSinkSpec:
2024-04-05 08:03:17 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_e2fd7a07-ae24-4d03-bfe7-1dca7e53c292.temp'.
2024-04-05 08:03:18 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_e2fd7a07-ae24-4d03-bfe7-1dca7e53c292.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_e2fd7a07-ae24-4d03-bfe7-1dca7e53c292.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
[info] - Saving the input data results in the same data
2024-04-05 08:03:21 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp'.
2024-04-05 08:03:21 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
2024-04-05 08:03:21 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp'.
2024-04-05 08:03:21 ERROR FileDataSink:93 - Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
	at org.tupol.spark.io.FileDataSink.$anonfun$write$12(FileDataSink.scala:115)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:112)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.FileDataSinkSpec.$anonfun$new$2(FileDataSinkSpec.scala:37)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.FileDataSinkSpec.runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSinkSpec.run(FileDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_89036364-0ea1-4ef8-a6fd-66a659a15b72.temp already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$8(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.Option.getOrElse(Option.scala:189)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$5(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$1(FileDataSink.scala:100)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:86)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-05 08:03:22 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-05 08:03:22 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3449a0b5-b683-4ee8-b6e8-bc3a6de932e0.temp'.
2024-04-05 08:03:22 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3449a0b5-b683-4ee8-b6e8-bc3a6de932e0.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3449a0b5-b683-4ee8-b6e8-bc3a6de932e0.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: 'Unchanged', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned results in the same data
2024-04-05 08:03:25 DEBUG FileDataSink:56 - Initializing the DataFrameWriter after repartitioning data to 1 partitions.
2024-04-05 08:03:25 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-05 08:03:25 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_15ee9fdb-7586-4487-8b6a-0dc0e26c5785.temp'.
2024-04-05 08:03:25 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_15ee9fdb-7586-4487-8b6a-0dc0e26c5785.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_15ee9fdb-7586-4487-8b6a-0dc0e26c5785.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: '1', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-05 08:03:28 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to bucket the data into 1 buckets using the following partition columns: int, string].
2024-04-05 08:03:28 DEBUG FileDataSink:56 - Buckets will be sorted by the following columns: int, string].
2024-04-05 08:03:28 INFO  FileDataSink:53 - Writing data to Hive as 'json' in the 'test_output_table' table due to the buckets configuration: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string]. Notice that the path parameter is used as a table name in this case.
2024-04-05 08:03:28 INFO  FileDataSink:53 - Successfully saved the data as 'json' to 'test_output_table' (Full configuration: path: 'test_output_table', format: 'json', save mode: 'overwrite', partitioning: None, bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: {}).
[info] - Saving the input bucketed results in the same data
[info] FileStreamDataSourceSpec:
2024-04-05 08:03:30 INFO  FileStreamDataSource:53 - Reading data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3178f6a9-d573-412b-b56b-47d2876b4c73.temp'.
2024-04-05 08:03:30 DEBUG FileStreamDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-05 08:03:30 INFO  FileStreamDataSource:53 - Successfully read the data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3178f6a9-d573-412b-b56b-47d2876b4c73.temp'
[info] - String messages should be written to the file stream and read back
[info] - Fail gracefully
[info] GenericFileStreamDataSourceSpec:
2024-04-05 08:03:36 INFO  GenericStreamDataSource:53 - Reading data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_c5259692-6a2b-4c22-a52e-cc4cc6a543bb.temp' }, schema: not specified'.
2024-04-05 08:03:36 INFO  GenericStreamDataSource:53 - Successfully read the data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_c5259692-6a2b-4c22-a52e-cc4cc6a543bb.temp' }, schema: not specified'
[info] - String messages should be written to the file stream and read back using a GenericStreamDataSource
[info] - Fail gracefully
[info] FormatTypeSpec:
[info] - fromString works on known types
[info] - fromString returns a Custom format type
[info] CsvFileDataSourceSpec:
2024-04-05 08:03:40 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:40 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-05 08:03:41 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-05 08:03:41 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:41 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-05 08:03:42 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - The number of records in the csv provided must be the same in the output result
2024-04-05 08:03:46 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'.
2024-04-05 08:03:46 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'
[info] - The number of records in multiple csv files provided must be the same in the output result
2024-04-05 08:03:47 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:47 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - User must be able to specify a schema 
2024-04-05 08:03:47 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:47 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-05 08:03:47 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:47 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-05 08:03:47 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'.
2024-04-05 08:03:47 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'
[info] - Providing a good csv schema fitting perfectly the data with the parsing option FAILFAST, expecting a success run and a successful dataframe materialization
2024-04-05 08:03:47 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:47 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:47 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-05 08:03:48 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:48 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:48 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-05 08:03:48 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-05 08:03:48 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-05 08:03:48 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-05 08:03:48 ERROR Executor:94 - Exception in task 0.0 in stage 41.0 (TID 441)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:395)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:304)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:249)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:391)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:272)
	... 25 more
2024-04-05 08:03:48 ERROR TaskSetManager:73 - Task 0 in stage 41.0 failed 1 times; aborting job
[info] - Providing a wrong csv schema with the parsing option FAILFAST, expecting a success run and a failed dataframe materialization
[info] GenericFileStreamDataSinkSpec:
2024-04-05 08:03:49 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8e1fac36-cdc1-4d89-b43c-53ba34e256e4.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_d5a77a39-f14c-47e1-a718-40b571c8afa7.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:03:49 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8e1fac36-cdc1-4d89-b43c-53ba34e256e4.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_d5a77a39-f14c-47e1-a718-40b571c8afa7.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-05 08:03:54 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9af33a30-7c24-49ad-86f4-1a2ad150fbad.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_5e1c770a-01ab-4972-b48d-e0577143ef7a.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:03:54 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9af33a30-7c24-49ad-86f4-1a2ad150fbad.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_5e1c770a-01ab-4972-b48d-e0577143ef7a.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] GenericSocketStreamDataSourceSpec:
2024-04-05 08:03:58 INFO  GenericStreamDataSource:53 - Reading data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'.
2024-04-05 08:03:58 INFO  GenericStreamDataSource:53 - Successfully read the data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'
[info] - String messages should be written to the socket stream and read back
[info] XmlFileDataSourceSpec:
2024-04-05 08:04:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-05 08:04:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-05 08:04:03 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-05 08:04:03 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:03 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:03 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
2024-04-05 08:04:03 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-05 08:04:03 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:03 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:03 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract the root element of a single file should yield a single result
2024-04-05 08:04:14 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-05 08:04:14 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'.
2024-04-05 08:04:14 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'
[info] - Extract the root element of multiple files should yield as many results as the number of files
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract elements that do not exist should return an empty result
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Infer simple schema
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-05 08:04:14 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-05 08:04:14 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-05 08:04:14 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-05 08:04:14 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-05 08:04:14 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:14 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-05 08:04:14 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
2024-04-05 08:04:15 ERROR Executor:94 - Exception in task 0.0 in stage 38.0 (TID 1636)
java.lang.IllegalArgumentException: Malformed line in FAILFAST mode
	at com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NumberFormatException: For input string: "abc"
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)
	at java.base/java.lang.Long.parseLong(Long.java:711)
	at java.base/java.lang.Long.parseLong(Long.java:836)
	at scala.collection.immutable.StringLike.toLong(StringLike.scala:315)
	at scala.collection.immutable.StringLike.toLong$(StringLike.scala:315)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:33)
	at com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:56)
	at com.databricks.spark.xml.util.TypeCast$.signSafeToLong(TypeCast.scala:277)
	at com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:183)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)
	... 21 more
2024-04-05 08:04:15 ERROR TaskSetManager:73 - Task 0 in stage 38.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] FileDataSourceSpec:
2024-04-05 08:04:15 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-05 08:04:15 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:04:15 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$2(FileDataSourceSpec.scala:19)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way;
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:784)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:767)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:590)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:415)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)
	at org.tupol.spark.io.FileDataSource.$anonfun$read$4(FileDataSource.scala:62)
	at scala.Option.getOrElse(Option.scala:189)
	at org.tupol.spark.io.FileDataSource.$anonfun$read$2(FileDataSource.scala:62)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:62)
	... 47 more
2024-04-05 08:04:15 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-05 08:04:15 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:04:15 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$3(FileDataSourceSpec.scala:21)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:21)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way;
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:784)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:767)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:590)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:415)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)
	at org.tupol.spark.io.FileDataSource.$anonfun$read$4(FileDataSource.scala:62)
	at scala.Option.getOrElse(Option.scala:189)
	at org.tupol.spark.io.FileDataSource.$anonfun$read$2(FileDataSource.scala:62)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:62)
	... 47 more
[info] - Loading the data fails if the file does not exist
2024-04-05 08:04:15 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-05 08:04:15 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-05 08:04:15 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
2024-04-05 08:04:15 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:04:15 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:04:15 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Loading a json data source works
[info] JdbcDataSinkSpec:
2024-04-05 08:04:15 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data
2024-04-05 08:04:16 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:04:16 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data with the overwrite save mode
2024-04-05 08:04:16 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:04:16 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in duplicated data with the append save mode
2024-04-05 08:04:16 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:04:17 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:04:17 ERROR JdbcDataSink:93 - Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$6(JdbcDataSink.scala:65)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:61)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.JdbcDataSink.write(JdbcDataSink.scala:50)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.JdbcDataSinkSpec.$anonfun$new$4(JdbcDataSinkSpec.scala:69)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSinkSpec.runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSinkSpec.run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Table or view 'test_table' already exists. SaveMode: ErrorIfExists.;
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:71)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$5(JdbcDataSink.scala:59)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:59)
	... 55 more
[info] - Saving the input data over the same table fails with default save mode
[info] GenericKafkaStreamDataSourceSpec:
2024-04-05 08:04:20 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-05 08:04:20 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] GenericDataSinkSpec:
2024-04-05 08:04:29 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_50bdbf6b-63e8-4609-8624-e1e9f61e4876.temp' }'.
2024-04-05 08:04:29 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_50bdbf6b-63e8-4609-8624-e1e9f61e4876.temp' }'.
[info] - Saving the input data results in the same data
2024-04-05 08:04:31 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp' }'.
2024-04-05 08:04:31 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp' }'.
2024-04-05 08:04:31 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp' }'.
2024-04-05 08:04:31 ERROR GenericDataSink:93 - Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp' }').
org.tupol.spark.io.DataSinkException: Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp' }').
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$4(GenericDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.GenericDataSinkSpec.$anonfun$new$2(GenericDataSinkSpec.scala:41)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.GenericDataSinkSpec.runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSinkSpec.run(GenericDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_bf8e2f8a-84a4-41ae-b26b-f5c2e96475e0.temp already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:121)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2(GenericDataSink.scala:57)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2$adapted(GenericDataSink.scala:57)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:57)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-05 08:04:31 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6e187b11-d152-47a8-ac76-2a1644a69fbe.temp' }'.
2024-04-05 08:04:32 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6e187b11-d152-47a8-ac76-2a1644a69fbe.temp' }'.
[info] - Saving the input partitioned results in the same data
2024-04-05 08:04:34 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_97d85331-1639-425f-85c5-c2d2c3cc4900.temp' }'.
2024-04-05 08:04:34 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_97d85331-1639-425f-85c5-c2d2c3cc4900.temp' }'.
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-05 08:04:36 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3acfd680-d9e5-436d-bd02-accc71112cc5.temp' }'.
2024-04-05 08:04:36 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3acfd680-d9e5-436d-bd02-accc71112cc5.temp' }'.
[info] - Saving the input bucketed results in the same data
[info] StreamingSourceConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileStreamDataSourceConfiguration
[info] - should overwrite existing options for GenericStreamDataSourceConfiguration
[info] - should overwrite existing options for KafkaStreamDataSourceConfiguration
[info] TextFileDataSourceSpec:
2024-04-05 08:04:37 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-05 08:04:37 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-05 08:04:38 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
2024-04-05 08:04:38 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/text/sample_schema.json'.
2024-04-05 08:04:38 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/text/sample_schema.json'.
2024-04-05 08:04:38 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-05 08:04:38 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-05 08:04:38 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
[info] - The number of records in the file provided and the schema must match
[info] AvroFileDataSourceSpec:
2024-04-05 08:04:39 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-05 08:04:39 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:04:39 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-05 08:04:40 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-05 08:04:40 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-05 08:04:40 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-05 08:04:40 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-05 08:04:40 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the schema must match
2024-04-05 08:04:41 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-05 08:04:41 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-05 08:04:41 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-05 08:04:41 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-05 08:04:41 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-05 08:04:41 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-05 08:04:41 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-05 08:04:41 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the other schema must match
[info] GenericStreamDataSinkSpec:
2024-04-05 08:04:43 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_eb85acdc-a72a-430f-9659-13cd5fbd4b53.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-05 08:04:44 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_eb85acdc-a72a-430f-9659-13cd5fbd4b53.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
[info] - Saving the input data as Json results in the same data
2024-04-05 08:04:54 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-05 08:04:54 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$8(GenericStreamDataSinkSpec.scala:72)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$7(GenericStreamDataSinkSpec.scala:72)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningServers(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningZooKeeper(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningKafka(GenericStreamDataSinkSpec.scala:17)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$6(GenericStreamDataSinkSpec.scala:71)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:126)
	at org.apache.hadoop.fs.Path.<init>(Path.java:134)
	at org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$createQuery$1(StreamingQueryManager.scala:244)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:243)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:367)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 78 more
[info] - Fail gracefully
[info] JsonFileDataSourceSpec:
2024-04-05 08:04:59 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-05 08:04:59 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-05 08:05:00 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-05 08:05:00 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:00 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:00 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
2024-04-05 08:05:00 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-05 08:05:00 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:00 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:00 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
[info] - Extract from a single file with a single record should yield a single result
2024-04-05 08:05:01 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-05 08:05:01 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-05 08:05:01 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
2024-04-05 08:05:01 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:01 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:01 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Extract from multiple files should yield as many results as the total number of records in the files
2024-04-05 08:05:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample.json'.
2024-04-05 08:05:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader inferring the schema.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample.json'
2024-04-05 08:05:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-05 08:05:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
[info] - Infer simple schema
2024-04-05 08:05:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-05 08:05:02 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-05 08:05:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-05 08:05:02 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-05 08:05:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-05 08:05:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-05 08:05:02 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-05 08:05:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-05 08:05:02 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-05 08:05:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-05 08:05:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
2024-04-05 08:05:02 ERROR Executor:94 - Exception in task 0.0 in stage 22.0 (TID 421)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:70)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:479)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:368)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:348)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:182)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:182)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:336)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$5(JacksonParser.scala:182)
	at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:387)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:89)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:88)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:336)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:88)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:454)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2541)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:449)
	... 24 more
2024-04-05 08:05:02 ERROR TaskSetManager:73 - Task 0 in stage 22.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] JdbcDataSourceSpec:
2024-04-05 08:05:02 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:05:02 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
2024-04-05 08:05:02 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:05:02 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
[info] - Reading the input data yields the correct result
2024-04-05 08:05:03 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-05 08:05:03 ERROR JdbcDataSource:93 - Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
org.tupol.spark.io.DataSourceException: Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$3(JdbcDataSource.scala:52)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:49)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$4(JdbcDataSourceSpec.scala:35)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$3(JdbcDataSourceSpec.scala:35)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSourceSpec.runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSourceSpec.run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.h2.jdbc.JdbcSQLException: Table "TEST_TABLE" not found; SQL statement:
SELECT * FROM test_table WHERE 1=0 [42102-197]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:357)
	at org.h2.message.DbException.get(DbException.java:179)
	at org.h2.message.DbException.get(DbException.java:155)
	at org.h2.command.Parser.readTableOrView(Parser.java:5920)
	at org.h2.command.Parser.readTableFilter(Parser.java:1430)
	at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:2138)
	at org.h2.command.Parser.parseSelectSimple(Parser.java:2287)
	at org.h2.command.Parser.parseSelectSub(Parser.java:2133)
	at org.h2.command.Parser.parseSelectUnion(Parser.java:1946)
	at org.h2.command.Parser.parseSelect(Parser.java:1919)
	at org.h2.command.Parser.parsePrepared(Parser.java:463)
	at org.h2.command.Parser.parse(Parser.java:335)
	at org.h2.command.Parser.parse(Parser.java:307)
	at org.h2.command.Parser.prepareCommand(Parser.java:278)
	at org.h2.engine.Session.prepareLocal(Session.java:611)
	at org.h2.engine.Session.prepareCommand(Session.java:549)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1247)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:76)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:304)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:58)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:221)
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$2(JdbcDataSource.scala:47)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:47)
	... 50 more
[info] - Reading the input data fails if table can not be found
[info] SourceConfigurationTest:
[info] schemaWithCorruptRecord
[info] - should return None if there is no schema defined
[info] - should return schema if columnNameOfCorruptRecord is not present in options
2024-04-05 08:05:03 DEBUG SourceConfigurationTest$$anon$3:56 - The 'columnNameOfCorruptRecord' was specified; adding column 'error' to the input schema.
[info] - should return schema with columnNameOfCorruptRecord extra column
[info] addOptions
[info] - should overwrite existing options for CsvSourceConfiguration
[info] - should overwrite existing options for XmlSourceConfiguration
[info] - should overwrite existing options for JsonSourceConfiguration
[info] - should overwrite existing options for ParquetSourceConfiguration
[info] - should overwrite existing options for OrcSourceConfiguration
[info] - should overwrite existing options for AvroSourceConfiguration
[info] - should overwrite existing options for TextSourceConfiguration
[info] - should overwrite existing options for JdbcSourceConfiguration
[info] - should overwrite existing options for GenericSourceConfiguration
[info] KafkaStreamDataSinkSpec:
2024-04-05 08:05:03 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a033519f-8fb5-4755-9e8b-2b2c33105873.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:05:03 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a033519f-8fb5-4755-9e8b-2b2c33105873.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-05 08:05:16 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:05:16 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSink.write(KafkaStreamDataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$8(KafkaStreamDataSinkSpec.scala:75)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$7(KafkaStreamDataSinkSpec.scala:75)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningServers(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningZooKeeper(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningKafka(KafkaStreamDataSinkSpec.scala:19)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$6(KafkaStreamDataSinkSpec.scala:74)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: checkpointLocation must be specified either through option("checkpointLocation", ...) or SparkSession.conf.set("spark.sql.streaming.checkpointLocation", ...);
	at org.apache.spark.sql.streaming.StreamingQueryManager.$anonfun$createQuery$5(StreamingQueryManager.scala:259)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:250)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:359)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:367)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 79 more
[info] - Fail gracefully
[info] ParquetFileDataSourceSpec:
2024-04-05 08:05:21 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-05 08:05:21 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-05 08:05:22 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
2024-04-05 08:05:22 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-05 08:05:22 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-05 08:05:22 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-05 08:05:22 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-05 08:05:22 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
[info] - The number of records in the file provided and the schema must match
[info] FileStreamDataSinkSpec:
2024-04-05 08:05:23 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_74c5ffc9-449e-4294-8cf5-b2a9511eb5c3.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8415a02b-adba-4d4e-a903-5a5efbd310d0.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:05:23 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_74c5ffc9-449e-4294-8cf5-b2a9511eb5c3.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8415a02b-adba-4d4e-a903-5a5efbd310d0.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-05 08:05:26 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_c2b345e1-c244-4d42-bd91-f6c2aaf182b5.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_876d1b0e-5721-4802-acd6-c69223ae21e0.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-05 08:05:26 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_c2b345e1-c244-4d42-bd91-f6c2aaf182b5.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_876d1b0e-5721-4802-acd6-c69223ae21e0.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] KafkaStreamDataSourceSpec:
2024-04-05 08:05:29 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-05 08:05:29 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] SinkConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileSinkConfiguration
[info] - should overwrite existing options for GenericSinkConfiguration
[info] OrcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-05 08:05:39 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:39 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:39 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:39 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:39 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSourceConfigurationSpec:
[info] - Successfully extract text FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract json FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract kafka FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract generic FileStreamDataSourceConfiguration out of a configuration string
[info] BucketsConfigurationSpec:
[info] - Successfully extract a full BucketsConfiguration
[info] - Successfully extract a partial BucketsConfiguration
[info] - Failed BucketsConfiguration, missing columns
[info] - Failed BucketsConfiguration, empty columns
[info] - Failed BucketsConfiguration, number = 0
[info] - Failed BucketsConfiguration, number < 0
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JdbcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with path schema
[info] - Parse configuration with explicit schema
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] XmlSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] GenericStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSourceConfiguration out of an empty string
[info] TextSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] KafkaStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract KafkaStreamDataSinkConfiguration out of an empty configuration string
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareDataSourceConfigurationSpec:
[info] - Successfully extract FileSourceConfiguration out of a configuration string
[info] - Successfully extract GenericSourceConfiguration out of a configuration string
[info] - Failed to extract FileSourceConfiguration if the path is not defined
[info] - Failed to extract FileSourceConfiguration if the format is not defined
[info] - Failed to extract FileSourceConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string containing only mandatory fields
[info] FileSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create FileSinkConfiguration using the simplified constructor
[info] - Failed to extract FileSinkConfiguration if the path is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not acceptable
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] TriggerExtractorSpec:
[info] - TriggerExtractor -> Trigger.Once()
[info] - TriggerExtractor -> Trigger.Continuous() Failure
[info] - TriggerExtractor -> Trigger.Continuous()
[info] - TriggerExtractor -> Trigger.ProcessingTime() Failure
[info] - TriggerExtractor -> Trigger.ProcessingTime()
[info] - TriggerExtractor Fails on unsupported trigger type
[info] - TriggerExtractor Fails on empty
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JsonSourceConfigurationSpec:
[info] Parse configuration
[info] - should work with schema and no options
[info] - should work without schema
[info] - should work with options
[info] - should fail when path is missing
[info] JdbcSinkConfigurationSpec:
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
[info] - Failed to extract JdbcSinkConfiguration out of an empty configuration string
[info] KafkaStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSourceConfiguration out of a configuration string
[info] - KafkaStreamDataSourceConfiguration required params overwrite the extra options when overlapping
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'kafkaBootstrapServers' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'subscription' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration out of an empty configuration string
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:43 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSinkConfigurationSpec:
[info] - Successfully extract a Text FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a Json FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract GenericStreamDataSinkConfiguration out of a configuration string
[info] - Failed to extract FormatAwareStreamingSinkConfiguration out of a configuration string
[info] PartitionsConfigurationSpec:
[info] - Successfully extract a full PartitionsConfiguration
[info] - Successfully extract a partial PartitionsConfiguration
[info] - Failed PartitionsConfiguration, missing columns
[info] - Failed PartitionsConfiguration, number = 0
[info] - Failed PartitionsConfiguration, number < 0
[info] AvroSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] FileSourceConfigurationSpec:
[info] - Successfully extract a text FileSourceConfiguration out of a configuration string
[info] - Successfully extract a csv FileSourceConfiguration out of a configuration string
[info] ParquetSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] GenericSinkConfigurationSpec:
[info] - Successfully extract GenericSinkConfiguration out of a configuration string
[info] - Successfully extract GenericSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create GenericSinkConfiguration using the simplified constructor
[info] - Successfully extract GenericSinkConfiguration even for a known format
[info] - Failed to extract GenericSinkConfiguration out of an empty configuration string
[info] FileStreamDataSourceConfigurationSpec:
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not supported
[info] - Failed to extract FileStreamDataSourceConfiguration if the path is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is incorrect
[info] - Failed to extract FileStreamDataSourceConfiguration out of an empty configuration string
[info] DataSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Successfully extract GenericSinkConfiguration out of a file configuration with a missing path
[info] - Successfully extract GenericSinkConfiguration out of a configuration with an unknown format
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] GenericSourceConfigurationSpec:
[info] - Parse configuration without options
[info] - Parse configuration with options
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with options and schema
[info] KafkaSubscriptionSpec:
[info] - KafkaSubscription.assign
[info] - KafkaSubscription.subscribe
[info] - KafkaSubscription.subscribePattern
[info] - KafkaSubscription Fails on unsupported type
[info] - KafkaSubscription Fails on empty
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] CsvSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] FileStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if the format is unsupported
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if options are missing
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] ExtendedSchemaExtractorSpec:
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from an external resource with a schema configuration path
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from an external resource without a schema configuration path
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource with a schema configuration path
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-05 08:05:44 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource without a schema configuration path
[info] - Load schema from config with a schema configuration path
[info] - Load schema from config without a schema configuration path
[info] - Fail to load schema from config without a schema configuration path
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-05 08:05:44 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:341)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-05 08:05:44 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-05 08:05:44 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:341)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Failure(org.tupol.spark.io.pureconf.errors$ConfigError: Cannot convert configuration to a scala.runtime.Nothing$. Failures are:
  at the root:
    - (String: 1) Cannot load 'this path does not actually exist': Unable to find 'this path does not actually exist' in the classpath.
    - (String: 1) Cannot convert '{"path":"this path does not actually exist"}' to StructType: Failed to convert the JSON string '{"path":"this path does not actually exist"}' to a data type..
)
[info] - Fail to load schema from a classpath resource
[info] FormatTypeSpec:
[info] - FormatTypeExtractor - custom
[info] - FormatTypeExtractor - avro
[info] - FormatTypeExtractor - xml
[info] - FormatTypeExtractor - xml compact
[info] GenericStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSinkConfiguration out of an empty string
[info] ScalaTest
[info] Run completed in 3 minutes, 15 seconds.
[info] Total number of tests run: 49
[info] Suites: completed 10, aborted 0
[info] Tests: succeeded 49, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 55, Failed 0, Errors 0, Passed 55
[info] ScalaTest
[info] Run completed in 3 minutes, 0 seconds.
[info] Total number of tests run: 96
[info] Suites: completed 27, aborted 0
[info] Tests: succeeded 96, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 96, Failed 0, Errors 0, Passed 96
[info] ScalaTest
[info] Run completed in 2 minutes, 44 seconds.
[info] Total number of tests run: 129
[info] Suites: completed 29, aborted 0
[info] Tests: succeeded 129, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 129, Failed 0, Errors 0, Passed 129
[success] Total time: 243 s (04:03), completed 5 Apr 2024, 08:05:45
[info] Forcing Scala version to 2.13.13 on all projects.
[info] Reapplying settings...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[warn] 
[warn] 	Note: Unresolved dependencies path:
[error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-streaming_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-streaming_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-sql_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-sql_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-avro_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-avro_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-core_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-core_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-mllib_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-mllib_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:246)
[error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$34(CoursierDependencyResolution.scala:215)
[error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)
[error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:215)
[error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)
[error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:52)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:102)
[error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:69)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:115)
[error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:115)
[error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:96)
[error] 	at sbt.util.Tracked$.$anonfun$inputChanged$1(Tracked.scala:150)
[error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:129)
[error] 	at sbt.Classpaths$.$anonfun$updateTask0$5(Defaults.scala:2947)
[error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)
[error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)
[error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:67)
[error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:281)
[error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:19)
[error] 	at sbt.Execute.work(Execute.scala:290)
[error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:281)
[error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178)
[error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:37)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
[error] 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[error] 	at java.base/java.lang.Thread.run(Thread.java:840)
[error] (spark-utils-core / update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-streaming_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-streaming_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-streaming_2.13/3.0.3/spark-streaming_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-sql_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-sql_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-sql_2.13/3.0.3/spark-sql_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-avro_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-avro_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-avro_2.13/3.0.3/spark-avro_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-core_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-core_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-core_2.13/3.0.3/spark-core_2.13-3.0.3.pom
[error] Error downloading org.apache.spark:spark-mllib_2.13:3.0.3
[error]   Not found
[error]   Not found
[error]   not found: /Users/olivertupran/.ivy2/local/org.apache.spark/spark-mllib_2.13/3.0.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/apache/spark/spark-mllib_2.13/3.0.3/spark-mllib_2.13-3.0.3.pom
[error] Total time: 2 s, completed 5 Apr 2024, 08:05:47
