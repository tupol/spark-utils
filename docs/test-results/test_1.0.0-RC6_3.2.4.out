[info] Loading settings for project global-plugins from sonatype.sbt,gpg.sbt ...
[info] Loading global plugins from /Users/olivertupran/.sbt/1.0/plugins
[info] Loading settings for project spark-utils-build from plugins.sbt ...
[info] Loading project definition from /Users/olivertupran/work/tupol/spark-utils/project
[info] Loading settings for project spark-utils from version.sbt,build.sbt ...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[success] Total time: 0 s, completed 4 Apr 2024, 19:41:17
[info] Forcing Scala version to 2.12.19 on all projects.
[info] Reapplying settings...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[info] Compiling 15 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/classes ...
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[info] Done compiling.
[info] Compiling 20 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.12/classes ...
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes ...
[info] Done compiling.
[info] Compiling 7 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.12/classes ...
[info] Done compiling.
[info] Compiling 27 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.12/test-classes ...
[info] + makeNameAvroCompliant.makeNameAvroCompliant should not change the name if the name is Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should change the name if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce shorter names if the name is not Avro compliant and replaceWith, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce same size names if the name is not Avro compliant and replaceWith is a single char, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should prepend the specified prefix if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should append the specified suffix if the name is not Avro compliant: OK, passed 100 tests.
[info] ConfigSpec:
[info] FuzzyTypesafeConfigBuilder.getConfiguration
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.whoami="app.param"
MockApp.param="param"
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-bbb05c25-1927-4317-9eaa-2fb8ea974dc9/app.conf is available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-bbb05c25-1927-4317-9eaa-2fb8ea974dc9/app.conf'
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
2024-04-04 19:42:14 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-04 19:42:14 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app params then defaults to the Spark app.conf file, then to the app.conf in the classpath and then to reference.conf
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockFun.param="param"
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-a575fdc9-dc32-4298-87f3-ce2389a28559/fun.conf is available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-a575fdc9-dc32-4298-87f3-ce2389a28559/fun.conf'
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/fun.conf is not available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'fun.conf'
2024-04-04 19:42:14 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-04 19:42:14 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app.conf then defaults to reference.conf
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param="param"
my.var="MYVAR"
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-b859e019-7388-44ec-84c3-d750a5b443a8/app.conf is available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-b859e019-7388-44ec-84c3-d750a5b443a8/app.conf'
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-04 19:42:14 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
[info] - should perform variable substitution
2024-04-04 19:42:15 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param=param
MockApp.my.var=MYVAR
MockApp.substitute.my-other-var=MY_OTHER_VAR
2024-04-04 19:42:15 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-ad1b550a-1339-47cb-bcfd-07c43cd6b500/MockApp/app2.conf is not available.
2024-04-04 19:42:15 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/MockApp/app2.conf is not available.
2024-04-04 19:42:15 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'MockApp/app2.conf'
[info] - should perform variable substitution and addition
[info] SimpleTypesafeConfigBuilder
[info] - should loads first the app params then defaults to app.conf file, then to the app.conf in the classpath and then to reference.conf
[info] - should defaults to reference.conf
[info] - should perform variable substitution
[info] SparkFunSpec:
2024-04-04 19:42:16 INFO  SparkFunSpec$MockFun$:62 - Running MockFun
2024-04-04 19:42:16 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:16 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-b2521b93-9a91-4c44-9df5-6a4d0758990f/application.conf is not available.
2024-04-04 19:42:16 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:16 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:16 DEBUG SparkFunSpec$MockFun$:97 - MockFun: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_fun",
    "whoami" : "./src/test/resources/reference.conf"
}

2024-04-04 19:42:17 INFO  SparkFunSpec$MockFun$:72 - MockFun: Job successfully completed.
[info] - SparkFun.main successfully completes
2024-04-04 19:42:17 INFO  SparkFunSpec$MockFunNoConfig:62 - Running MockFunNoConfig
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-31474f11-cf6b-4212-9247-580076d17729/application.conf is not available.
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:17 ERROR SparkFunSpec$MockFunNoConfig:101 - MockFunNoConfig: Failed to load application configuration from the MockFunNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunNoConfig'
2024-04-04 19:42:17 INFO  SparkFunSpec$MockFunNoConfig:72 - MockFunNoConfig: Job successfully completed.
[info] - SparkFun.main successfully completes with no configuration expected
2024-04-04 19:42:17 INFO  SparkFunSpec$MockFunFailure:62 - Running MockFunFailure
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-7e5dcd3c-d741-482f-a073-46e3d44f13a8/application.conf is not available.
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:17 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:17 ERROR SparkFunSpec$MockFunFailure:101 - MockFunFailure: Failed to load application configuration from the MockFunFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunFailure'
2024-04-04 19:42:17 ERROR SparkFunSpec$MockFunFailure:73 - MockFunFailure: Job failed.
org.tupol.spark.SparkFunSpec$MockApException
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:54)
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:53)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkFun.main(SparkFun.scala:20)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$6(SparkFunSpec.scala:31)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$5(SparkFunSpec.scala:31)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkFunSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSpec.scala:11)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkFunSpec.runTest(SparkFunSpec.scala:11)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkFun.main fails gracefully if SparkFun.run fails
[info] - SparkFun.appName gets the simple class name
[info] MakeNameAvroCompliantSpec:
[info] - makeNameAvroCompliant removes the non-compliant chars
[info] - makeNameAvroCompliant replaces the first non-compliant char
[info] - makeNameAvroCompliant does not replace the first non-compliant char if a prefix is specified
[info] - makeNameAvroCompliant does not work with empty strings
[info] - makeNameAvroCompliant reports non-compliant prefix
[info] - makeNameAvroCompliant reports non-compliant replaceWith
[info] - makeNameAvroCompliant reports non-compliant replaceWith first char if prefix is not specified
[info] - makeNameAvroCompliant works with non-compliant replaceWith first char if prefix is specified
[info] - makeNameAvroCompliant reports non-compliant suffix
[info] Done compiling.
[info] - makeNameAvroCompliant successfully cleans the schema
[info] KeyValueDatasetOpsTest:
[info] mapValues
[info] - should map only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] flatMapValues
[info] - should flatMap only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] SchemaOpsSpec:
[info] - mapFields & checkAllFields
[info] - checkAllFields should fail inside array
[info] - checkAllFields should fail inside map
[info] - checkAnyFields plainly
[info] - checkAnyFields inside arrays
[info] - checkAnyFields inside maps
[info] DataFrameOpsSpec:
[info] - flattenFields on a flat DataFrame should produce no changes.
[info] - flattenFields on a structured DataFrame should flatten it.
2024-04-04 19:42:31 ERROR package:93 - Failed to load text resource from ''.
java.lang.IllegalArgumentException: Cannot load a text resource from an empty path.
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:47)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.utils.UtilsSpec.$anonfun$new$1(UtilsSpec.scala:14)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from local file '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URI '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URL '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from classpath '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:42:31 ERROR package:93 - Failed to load text resource from '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
java.lang.IllegalArgumentException: Unable to find '/unknown/path/leading/to/unknown/file/s1n.ci7y' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.utils.UtilsSpec.$anonfun$new$2(UtilsSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from local file 'utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URI 'utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URL 'utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from classpath 'utils/sample-text.resource'.
[info] UtilsSpec:
[info] - fuzzyLoadTextResourceFile fails while trying to load an empty path
[info] - fuzzyLoadTextResourceFile fails while trying to load a path that does not exist anywhere
[info] - fuzzyLoadTextResourceFile successfully loads a text from a local path
2024-04-04 19:42:31 DEBUG package:56 - Successfully loaded text resource from classpath 'utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from local file 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Successfully loaded resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from local file 'http://info.cern.ch'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URI 'http://info.cern.ch'.
2024-04-04 19:42:31 DEBUG package:56 - Try loading text resource from URL 'http://info.cern.ch'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from the class path
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URI
2024-04-04 19:42:31 DEBUG package:56 - Successfully loaded resource from URL 'http://info.cern.ch'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URL
[info] MapOpsRowOpsSpec:
[info] Done compiling.
[info] - Converting a map to a row and the conversion back from a row to a map
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.12/test-classes ...
[info] DatasetOpsTest:
[info] withTupledColumn
[info] - should return a tuple of input and column with a simple dataset of 1 value
[info] - should return a tuple of input and column with a simple dataset of 2 values
[info] - should return a tuple of input and column with a simple dataset of nested values
[info] - should return an empty dataset for an empty dataset
[info] - should fail if the specified column type does not match the actual column type
[info] SparkAppSpec:
2024-04-04 19:42:33 INFO  SparkAppSpec$MockApp$:62 - Running MockApp
2024-04-04 19:42:33 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:33 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-b33f8d5d-e190-4564-a0c6-cc2599852c51/application.conf is not available.
2024-04-04 19:42:33 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:33 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:33 DEBUG SparkAppSpec$MockApp$:97 - MockApp: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_app",
    "whoami" : "./src/test/resources/reference.conf"
}

2024-04-04 19:42:33 INFO  SparkAppSpec$MockApp$:72 - MockApp: Job successfully completed.
[info] - SparkApp.main successfully completes
2024-04-04 19:42:34 INFO  SparkAppSpec$MockAppNoConfig:62 - Running MockAppNoConfig
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-4d1847cd-c674-4522-be18-165add42ac27/application.conf is not available.
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:34 ERROR SparkAppSpec$MockAppNoConfig:101 - MockAppNoConfig: Failed to load application configuration from the MockAppNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppNoConfig'
2024-04-04 19:42:34 INFO  SparkAppSpec$MockAppNoConfig:72 - MockAppNoConfig: Job successfully completed.
[info] - SparkApp.main successfully completes with no configuration expected
2024-04-04 19:42:34 INFO  SparkAppSpec$MockAppFailure:62 - Running MockAppFailure
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-11af63d7-80fe-42c4-b2d7-b5c5fa30ab5a/userFiles-1bca19eb-523d-4c6b-91c4-1ba4df82ed06/application.conf is not available.
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:42:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:42:34 ERROR SparkAppSpec$MockAppFailure:101 - MockAppFailure: Failed to load application configuration from the MockAppFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.12/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppFailure'
2024-04-04 19:42:34 ERROR SparkAppSpec$MockAppFailure:73 - MockAppFailure: Job failed.
org.tupol.spark.SparkAppSpec$MockApException
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:56)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.main(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$6(SparkAppSpec.scala:33)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$5(SparkAppSpec.scala:33)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkAppSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkAppSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkAppSpec.runTest(SparkAppSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkApp.main fails gracefully if SparkApp.run fails
[info] - SparkApp.appName gets the simple class name
[info] GenericDataSourceSpec:
2024-04-04 19:42:39 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-04 19:42:40 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:42:40 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$2(GenericDataSourceSpec.scala:23)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:74)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2024-04-04 19:42:40 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-04 19:42:40 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:42:40 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$3(GenericDataSourceSpec.scala:25)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:25)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:74)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[info] - Loading the data fails if the file does not exist
2024-04-04 19:42:40 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-04 19:42:40 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
[info] Done compiling.
2024-04-04 19:42:44 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
2024-04-04 19:42:46 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:42:46 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:42:46 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-04 19:42:46 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:42:46 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
[info] - The number of records in the file provided and the schema must match
2024-04-04 19:42:51 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:42:51 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:42:51 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-04 19:42:51 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:42:51 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2024-04-04 19:42:52 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-04 19:42:52 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:42:52 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
[info] - The number of records in the file provided and the other schema must match
[info] addOptions
[info] - should leave the empty options unchanged
[info] - should leave the options unchanged
[info] - should add extra options
[info] - should override options
[info] withSchema
[info] - should set a new schema
[info] - should change an existing schema
[info] - should remove an existing schema
[info] OrcFileDataSourceSpec:
2024-04-04 19:42:55 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-04 19:42:55 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-04 19:42:56 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
2024-04-04 19:42:56 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-04 19:42:56 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-04 19:42:56 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-04 19:42:56 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-04 19:42:56 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
[info] - The number of records in the file provided and the schema must match
[info] StreamingSinkConfigurationSpec:
[info] addOptions
[info] - should overwrite existing options for GenericStreamDataSinkConfiguration
[info] - should overwrite existing options for FileStreamDataSinkConfiguration
[info] - should overwrite existing options for KafkaStreamDataSinkConfiguration
[info] FileDataSinkSpec:
2024-04-04 19:42:59 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_643dea2a-f479-4abf-9307-76232a3dd32c.temp'.
2024-04-04 19:43:00 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_643dea2a-f479-4abf-9307-76232a3dd32c.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_643dea2a-f479-4abf-9307-76232a3dd32c.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
[info] - Saving the input data results in the same data
2024-04-04 19:43:05 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp'.
2024-04-04 19:43:06 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
2024-04-04 19:43:06 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp'.
2024-04-04 19:43:06 ERROR FileDataSink:93 - Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
	at org.tupol.spark.io.FileDataSink.$anonfun$write$12(FileDataSink.scala:115)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:112)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.FileDataSinkSpec.$anonfun$new$2(FileDataSinkSpec.scala:37)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.FileDataSinkSpec.runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSinkSpec.run(FileDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2a4f0b07-1b92-48b8-bc9e-fb3c2b5be1bf.temp already exists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1142)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$8(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.Option.getOrElse(Option.scala:189)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$5(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$1(FileDataSink.scala:100)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:86)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-04 19:43:06 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-04 19:43:06 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6372e83e-f489-4caa-a0ee-3de4805929ce.temp'.
2024-04-04 19:43:07 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6372e83e-f489-4caa-a0ee-3de4805929ce.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6372e83e-f489-4caa-a0ee-3de4805929ce.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: 'Unchanged', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned results in the same data
2024-04-04 19:43:11 DEBUG FileDataSink:56 - Initializing the DataFrameWriter after repartitioning data to 1 partitions.
2024-04-04 19:43:11 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-04 19:43:11 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_47dd9076-789a-4033-9134-dec2ff2afe27.temp'.
2024-04-04 19:43:11 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_47dd9076-789a-4033-9134-dec2ff2afe27.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_47dd9076-789a-4033-9134-dec2ff2afe27.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: '1', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-04 19:43:13 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to bucket the data into 1 buckets using the following partition columns: int, string].
2024-04-04 19:43:13 DEBUG FileDataSink:56 - Buckets will be sorted by the following columns: int, string].
2024-04-04 19:43:13 INFO  FileDataSink:53 - Writing data to Hive as 'json' in the 'test_output_table' table due to the buckets configuration: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string]. Notice that the path parameter is used as a table name in this case.
2024-04-04 19:43:14 INFO  FileDataSink:53 - Successfully saved the data as 'json' to 'test_output_table' (Full configuration: path: 'test_output_table', format: 'json', save mode: 'overwrite', partitioning: None, bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: {}).
[info] - Saving the input bucketed results in the same data
[info] FileStreamDataSourceSpec:
2024-04-04 19:43:16 INFO  FileStreamDataSource:53 - Reading data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_15b8392a-95bc-40cd-abb3-a381fc76f312.temp'.
2024-04-04 19:43:16 DEBUG FileStreamDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:43:16 INFO  FileStreamDataSource:53 - Successfully read the data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_15b8392a-95bc-40cd-abb3-a381fc76f312.temp'
[info] - String messages should be written to the file stream and read back
[info] - Fail gracefully
[info] GenericFileStreamDataSourceSpec:
2024-04-04 19:43:23 INFO  GenericStreamDataSource:53 - Reading data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_ded7ff8d-f3f9-4972-a2cf-c5a8009279b4.temp' }, schema: not specified'.
2024-04-04 19:43:23 INFO  GenericStreamDataSource:53 - Successfully read the data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_ded7ff8d-f3f9-4972-a2cf-c5a8009279b4.temp' }, schema: not specified'
[info] - String messages should be written to the file stream and read back using a GenericStreamDataSource
[info] - Fail gracefully
[info] FormatTypeSpec:
[info] - fromString works on known types
[info] - fromString returns a Custom format type
[info] CsvFileDataSourceSpec:
2024-04-04 19:43:26 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:26 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:43:27 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-04 19:43:28 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:28 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:43:29 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - The number of records in the csv provided must be the same in the output result
2024-04-04 19:43:30 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'.
2024-04-04 19:43:30 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'
2024-04-04 19:43:31 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:31 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
[info] - The number of records in multiple csv files provided must be the same in the output result
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - User must be able to specify a schema 
2024-04-04 19:43:31 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:31 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-04 19:43:31 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:31 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-04 19:43:31 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'.
2024-04-04 19:43:31 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'
[info] - Providing a good csv schema fitting perfectly the data with the parsing option FAILFAST, expecting a success run and a successful dataframe materialization
2024-04-04 19:43:31 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:31 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:31 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-04 19:43:32 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:32 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:32 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-04 19:43:32 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:43:32 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:43:32 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-04 19:43:32 ERROR Executor:94 - Exception in task 0.0 in stage 59.0 (TID 43)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1236)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:410)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:319)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:264)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:406)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1037)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:287)
	... 25 more
2024-04-04 19:43:32 ERROR TaskSetManager:73 - Task 0 in stage 59.0 failed 1 times; aborting job
[info] - Providing a wrong csv schema with the parsing option FAILFAST, expecting a success run and a failed dataframe materialization
[info] GenericFileStreamDataSinkSpec:
2024-04-04 19:43:32 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7e7616ab-985a-4525-b8da-83fab37da36b.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_977f7c8e-91ab-4ec3-b215-bfc142bd1d64.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:43:33 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7e7616ab-985a-4525-b8da-83fab37da36b.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_977f7c8e-91ab-4ec3-b215-bfc142bd1d64.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:43:36 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a20b02f4-ee23-4944-8f8b-df6f52c5e03a.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3c3b06c0-a080-47a4-82d4-3a19e59b92a5.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:43:36 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a20b02f4-ee23-4944-8f8b-df6f52c5e03a.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3c3b06c0-a080-47a4-82d4-3a19e59b92a5.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] GenericSocketStreamDataSourceSpec:
2024-04-04 19:43:38 INFO  GenericStreamDataSource:53 - Reading data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'.
2024-04-04 19:43:38 INFO  GenericStreamDataSource:53 - Successfully read the data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'
[info] - String messages should be written to the socket stream and read back
[info] XmlFileDataSourceSpec:
2024-04-04 19:43:42 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:43:42 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:43:42 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:43:42 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:42 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:42 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
2024-04-04 19:43:43 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:43:43 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:43 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:43 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract the root element of a single file should yield a single result
2024-04-04 19:43:51 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:43:51 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:43:51 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'.
2024-04-04 19:43:51 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:51 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:51 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'
[info] - Extract the root element of multiple files should yield as many results as the number of files
2024-04-04 19:43:52 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:43:52 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-04 19:43:53 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract elements that do not exist should return an empty result
2024-04-04 19:43:54 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:43:54 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-04 19:43:54 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Infer simple schema
2024-04-04 19:43:55 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:43:55 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:55 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:55 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-04 19:43:56 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:43:56 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-04 19:43:56 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:56 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-04 19:43:57 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:43:57 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:57 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:57 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-04 19:43:59 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:43:59 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:43:59 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:43:59 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
2024-04-04 19:44:00 ERROR Executor:94 - Exception in task 0.0 in stage 54.0 (TID 40)
java.lang.IllegalArgumentException: Malformed line in FAILFAST mode
	at com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NumberFormatException: For input string: "abc"
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)
	at java.base/java.lang.Long.parseLong(Long.java:711)
	at java.base/java.lang.Long.parseLong(Long.java:836)
	at scala.collection.immutable.StringLike.toLong(StringLike.scala:315)
	at scala.collection.immutable.StringLike.toLong$(StringLike.scala:315)
	at scala.collection.immutable.StringOps.toLong(StringOps.scala:33)
	at com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:56)
	at com.databricks.spark.xml.util.TypeCast$.signSafeToLong(TypeCast.scala:277)
	at com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:183)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)
	... 21 more
2024-04-04 19:44:00 ERROR TaskSetManager:73 - Task 0 in stage 54.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] FileDataSourceSpec:
2024-04-04 19:44:00 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-04 19:44:00 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:44:00 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$2(FileDataSourceSpec.scala:19)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:74)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2024-04-04 19:44:00 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-04 19:44:00 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:44:00 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$3(FileDataSourceSpec.scala:21)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:21)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:42)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:74)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[info] - Loading the data fails if the file does not exist
2024-04-04 19:44:00 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-04 19:44:00 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-04 19:44:00 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
2024-04-04 19:44:00 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:44:00 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:44:00 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Loading a json data source works
[info] JdbcDataSinkSpec:
2024-04-04 19:44:01 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data
2024-04-04 19:44:01 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:44:02 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data with the overwrite save mode
2024-04-04 19:44:02 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:44:02 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in duplicated data with the append save mode
2024-04-04 19:44:03 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:44:03 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:44:03 ERROR JdbcDataSink:93 - Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$6(JdbcDataSink.scala:65)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:61)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.JdbcDataSink.write(JdbcDataSink.scala:50)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.JdbcDataSinkSpec.$anonfun$new$4(JdbcDataSinkSpec.scala:69)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSinkSpec.runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSinkSpec.run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Table or view 'test_table' already exists. SaveMode: ErrorIfExists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.tableOrViewAlreadyExistsError(QueryCompilationErrors.scala:2098)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:72)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$5(JdbcDataSink.scala:59)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:59)
	... 55 more
[info] - Saving the input data over the same table fails with default save mode
[info] GenericKafkaStreamDataSourceSpec:
2024-04-04 19:44:09 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-04 19:44:09 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] GenericDataSinkSpec:
2024-04-04 19:44:23 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_933c4055-42f3-4cff-a2ff-06b991ce857c.temp' }'.
2024-04-04 19:44:24 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_933c4055-42f3-4cff-a2ff-06b991ce857c.temp' }'.
[info] - Saving the input data results in the same data
2024-04-04 19:44:26 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp' }'.
2024-04-04 19:44:26 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp' }'.
2024-04-04 19:44:26 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp' }'.
2024-04-04 19:44:26 ERROR GenericDataSink:93 - Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp' }').
org.tupol.spark.io.DataSinkException: Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp' }').
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$4(GenericDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.GenericDataSinkSpec.$anonfun$new$2(GenericDataSinkSpec.scala:41)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.GenericDataSinkSpec.runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSinkSpec.run(GenericDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8b11893-84e2-48c5-886c-0f53d258f84c.temp already exists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1142)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2(GenericDataSink.scala:57)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2$adapted(GenericDataSink.scala:57)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:57)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-04 19:44:26 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_14b6ce60-69d0-4bb8-b59e-f34027d3b298.temp' }'.
2024-04-04 19:44:27 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_14b6ce60-69d0-4bb8-b59e-f34027d3b298.temp' }'.
[info] - Saving the input partitioned results in the same data
2024-04-04 19:44:30 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a8537958-c0c3-47d1-988d-6c8eddb271f1.temp' }'.
2024-04-04 19:44:31 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a8537958-c0c3-47d1-988d-6c8eddb271f1.temp' }'.
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-04 19:44:33 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9815ff83-6285-4736-9f6a-010c297b1afe.temp' }'.
2024-04-04 19:44:33 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9815ff83-6285-4736-9f6a-010c297b1afe.temp' }'.
[info] - Saving the input bucketed results in the same data
[info] StreamingSourceConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileStreamDataSourceConfiguration
[info] - should overwrite existing options for GenericStreamDataSourceConfiguration
[info] - should overwrite existing options for KafkaStreamDataSourceConfiguration
[info] TextFileDataSourceSpec:
2024-04-04 19:44:34 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-04 19:44:34 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:44:35 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
2024-04-04 19:44:35 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/text/sample_schema.json'.
2024-04-04 19:44:35 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/text/sample_schema.json'.
2024-04-04 19:44:35 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-04 19:44:35 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:44:35 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
[info] - The number of records in the file provided and the schema must match
[info] AvroFileDataSourceSpec:
2024-04-04 19:44:38 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:44:38 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:44:38 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-04 19:44:38 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:44:38 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:44:38 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:44:38 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:44:38 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the schema must match
2024-04-04 19:44:39 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:44:39 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:44:39 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:44:39 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:44:39 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-04 19:44:40 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:44:40 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:44:40 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the other schema must match
[info] GenericStreamDataSinkSpec:
2024-04-04 19:44:44 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_63106961-e7e7-4588-9813-1636cb611c21.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-04 19:44:44 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_63106961-e7e7-4588-9813-1636cb611c21.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:44:52 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-04 19:44:52 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$8(GenericStreamDataSinkSpec.scala:72)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$7(GenericStreamDataSinkSpec.scala:72)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningServers(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningZooKeeper(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningKafka(GenericStreamDataSinkSpec.scala:17)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$6(GenericStreamDataSinkSpec.scala:71)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:172)
	at org.apache.hadoop.fs.Path.<init>(Path.java:184)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.$anonfun$resolveCheckpointLocation$1(ResolveWriteToStream.scala:70)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:69)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:258)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:423)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:402)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 78 more
[info] - Fail gracefully
[info] JsonFileDataSourceSpec:
2024-04-04 19:44:57 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:44:57 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:44:57 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-04 19:44:57 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:44:57 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:44:57 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
2024-04-04 19:44:58 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-04 19:44:58 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:44:58 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:44:58 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
[info] - Extract from a single file with a single record should yield a single result
2024-04-04 19:45:02 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:45:02 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:45:02 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
2024-04-04 19:45:02 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:45:02 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:45:02 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Extract from multiple files should yield as many results as the total number of records in the files
2024-04-04 19:45:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample.json'.
2024-04-04 19:45:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader inferring the schema.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample.json'
2024-04-04 19:45:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:45:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
[info] - Infer simple schema
2024-04-04 19:45:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:45:03 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:45:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-04 19:45:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:45:03 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-04 19:45:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-04 19:45:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:45:03 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:45:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-04 19:45:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:45:03 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:45:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:45:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
2024-04-04 19:45:04 ERROR Executor:94 - Exception in task 0.0 in stage 33.0 (TID 23)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1236)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:507)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:974)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:387)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:370)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:358)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$5(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:409)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:358)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:482)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2742)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:477)
	... 24 more
2024-04-04 19:45:04 ERROR TaskSetManager:73 - Task 0 in stage 33.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] JdbcDataSourceSpec:
2024-04-04 19:45:04 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:45:04 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
2024-04-04 19:45:04 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:45:04 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
[info] - Reading the input data yields the correct result
2024-04-04 19:45:04 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:45:04 ERROR JdbcDataSource:93 - Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
org.tupol.spark.io.DataSourceException: Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$3(JdbcDataSource.scala:52)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:49)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$4(JdbcDataSourceSpec.scala:35)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$3(JdbcDataSourceSpec.scala:35)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSourceSpec.runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSourceSpec.run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.h2.jdbc.JdbcSQLException: Table "TEST_TABLE" not found; SQL statement:
SELECT * FROM test_table WHERE 1=0 [42102-197]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:357)
	at org.h2.message.DbException.get(DbException.java:179)
	at org.h2.message.DbException.get(DbException.java:155)
	at org.h2.command.Parser.readTableOrView(Parser.java:5920)
	at org.h2.command.Parser.readTableFilter(Parser.java:1430)
	at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:2138)
	at org.h2.command.Parser.parseSelectSimple(Parser.java:2287)
	at org.h2.command.Parser.parseSelectSub(Parser.java:2133)
	at org.h2.command.Parser.parseSelectUnion(Parser.java:1946)
	at org.h2.command.Parser.parseSelect(Parser.java:1919)
	at org.h2.command.Parser.parsePrepared(Parser.java:463)
	at org.h2.command.Parser.parse(Parser.java:335)
	at org.h2.command.Parser.parse(Parser.java:307)
	at org.h2.command.Parser.prepareCommand(Parser.java:278)
	at org.h2.engine.Session.prepareLocal(Session.java:611)
	at org.h2.engine.Session.prepareCommand(Session.java:549)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1247)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:76)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:304)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:64)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:57)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:36)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$2(JdbcDataSource.scala:47)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:47)
	... 50 more
[info] - Reading the input data fails if table can not be found
[info] SourceConfigurationTest:
[info] schemaWithCorruptRecord
[info] - should return None if there is no schema defined
[info] - should return schema if columnNameOfCorruptRecord is not present in options
2024-04-04 19:45:04 DEBUG SourceConfigurationTest$$anon$3:56 - The 'columnNameOfCorruptRecord' was specified; adding column 'error' to the input schema.
[info] - should return schema with columnNameOfCorruptRecord extra column
[info] addOptions
[info] - should overwrite existing options for CsvSourceConfiguration
[info] - should overwrite existing options for XmlSourceConfiguration
[info] - should overwrite existing options for JsonSourceConfiguration
[info] - should overwrite existing options for ParquetSourceConfiguration
[info] - should overwrite existing options for OrcSourceConfiguration
[info] - should overwrite existing options for AvroSourceConfiguration
[info] - should overwrite existing options for TextSourceConfiguration
[info] - should overwrite existing options for JdbcSourceConfiguration
[info] - should overwrite existing options for GenericSourceConfiguration
[info] KafkaStreamDataSinkSpec:
2024-04-04 19:45:05 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6893cf40-b81c-4ac2-8947-3fe0a22e3f6b.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:45:05 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6893cf40-b81c-4ac2-8947-3fe0a22e3f6b.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:45:13 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:45:13 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSink.write(KafkaStreamDataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$8(KafkaStreamDataSinkSpec.scala:75)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$7(KafkaStreamDataSinkSpec.scala:75)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningServers(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningZooKeeper(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningKafka(KafkaStreamDataSinkSpec.scala:19)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$6(KafkaStreamDataSinkSpec.scala:74)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException:  checkpointLocation must be specified either through option("checkpointLocation", ...) or SparkSession.conf.set("spark.sql.streaming.checkpointLocation", ...)        
	at org.apache.spark.sql.errors.QueryCompilationErrors$.checkpointLocationNotSpecifiedError(QueryCompilationErrors.scala:2151)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.$anonfun$resolveCheckpointLocation$5(ResolveWriteToStream.scala:85)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:76)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:258)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:423)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:402)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 79 more
[info] - Fail gracefully
[info] ParquetFileDataSourceSpec:
2024-04-04 19:45:19 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-04 19:45:19 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-04 19:45:19 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
2024-04-04 19:45:19 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-04 19:45:19 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-04 19:45:19 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-04 19:45:19 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-04 19:45:19 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
[info] - The number of records in the file provided and the schema must match
[info] FileStreamDataSinkSpec:
2024-04-04 19:45:20 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_30d37456-d437-4868-bf5c-30f4446e1fed.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_14560276-f666-453c-b89b-1457993fe092.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:45:20 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_30d37456-d437-4868-bf5c-30f4446e1fed.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_14560276-f666-453c-b89b-1457993fe092.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:45:22 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2bbe228a-7fa2-4386-b799-85d0993c59e1.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8604172-f034-44bb-81c3-2d6899e7da6d.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:45:22 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_2bbe228a-7fa2-4386-b799-85d0993c59e1.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_b8604172-f034-44bb-81c3-2d6899e7da6d.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] KafkaStreamDataSourceSpec:
2024-04-04 19:45:28 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-04 19:45:28 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] SinkConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileSinkConfiguration
[info] - should overwrite existing options for GenericSinkConfiguration
[info] OrcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-04 19:45:40 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:40 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:40 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:40 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:40 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSourceConfigurationSpec:
[info] - Successfully extract text FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract json FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract kafka FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract generic FileStreamDataSourceConfiguration out of a configuration string
[info] BucketsConfigurationSpec:
[info] - Successfully extract a full BucketsConfiguration
[info] - Successfully extract a partial BucketsConfiguration
[info] - Failed BucketsConfiguration, missing columns
[info] - Failed BucketsConfiguration, empty columns
[info] - Failed BucketsConfiguration, number = 0
[info] - Failed BucketsConfiguration, number < 0
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JdbcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with path schema
[info] - Parse configuration with explicit schema
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] XmlSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] GenericStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSourceConfiguration out of an empty string
[info] TextSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] KafkaStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract KafkaStreamDataSinkConfiguration out of an empty configuration string
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareDataSourceConfigurationSpec:
[info] - Successfully extract FileSourceConfiguration out of a configuration string
[info] - Successfully extract GenericSourceConfiguration out of a configuration string
[info] - Failed to extract FileSourceConfiguration if the path is not defined
[info] - Failed to extract FileSourceConfiguration if the format is not defined
[info] - Failed to extract FileSourceConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string containing only mandatory fields
[info] FileSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create FileSinkConfiguration using the simplified constructor
[info] - Failed to extract FileSinkConfiguration if the path is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not acceptable
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] TriggerExtractorSpec:
[info] - TriggerExtractor -> Trigger.Once()
[info] - TriggerExtractor -> Trigger.Continuous() Failure
[info] - TriggerExtractor -> Trigger.Continuous()
[info] - TriggerExtractor -> Trigger.ProcessingTime() Failure
[info] - TriggerExtractor -> Trigger.ProcessingTime()
[info] - TriggerExtractor Fails on unsupported trigger type
[info] - TriggerExtractor Fails on empty
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JsonSourceConfigurationSpec:
[info] Parse configuration
[info] - should work with schema and no options
[info] - should work without schema
[info] - should work with options
[info] - should fail when path is missing
[info] JdbcSinkConfigurationSpec:
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
[info] - Failed to extract JdbcSinkConfiguration out of an empty configuration string
[info] KafkaStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSourceConfiguration out of a configuration string
[info] - KafkaStreamDataSourceConfiguration required params overwrite the extra options when overlapping
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'kafkaBootstrapServers' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'subscription' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration out of an empty configuration string
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:45 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSinkConfigurationSpec:
[info] - Successfully extract a Text FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a Json FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract GenericStreamDataSinkConfiguration out of a configuration string
[info] - Failed to extract FormatAwareStreamingSinkConfiguration out of a configuration string
[info] PartitionsConfigurationSpec:
[info] - Successfully extract a full PartitionsConfiguration
[info] - Successfully extract a partial PartitionsConfiguration
[info] - Failed PartitionsConfiguration, missing columns
[info] - Failed PartitionsConfiguration, number = 0
[info] - Failed PartitionsConfiguration, number < 0
[info] AvroSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] FileSourceConfigurationSpec:
[info] - Successfully extract a text FileSourceConfiguration out of a configuration string
[info] - Successfully extract a csv FileSourceConfiguration out of a configuration string
[info] ParquetSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] GenericSinkConfigurationSpec:
[info] - Successfully extract GenericSinkConfiguration out of a configuration string
[info] - Successfully extract GenericSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create GenericSinkConfiguration using the simplified constructor
[info] - Successfully extract GenericSinkConfiguration even for a known format
[info] - Failed to extract GenericSinkConfiguration out of an empty configuration string
[info] FileStreamDataSourceConfigurationSpec:
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not supported
[info] - Failed to extract FileStreamDataSourceConfiguration if the path is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is incorrect
[info] - Failed to extract FileStreamDataSourceConfiguration out of an empty configuration string
[info] DataSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Successfully extract GenericSinkConfiguration out of a file configuration with a missing path
[info] - Successfully extract GenericSinkConfiguration out of a configuration with an unknown format
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] GenericSourceConfigurationSpec:
[info] - Parse configuration without options
[info] - Parse configuration with options
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with options and schema
[info] KafkaSubscriptionSpec:
[info] - KafkaSubscription.assign
[info] - KafkaSubscription.subscribe
[info] - KafkaSubscription.subscribePattern
[info] - KafkaSubscription Fails on unsupported type
[info] - KafkaSubscription Fails on empty
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] CsvSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] FileStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if the format is unsupported
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if options are missing
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] ExtendedSchemaExtractorSpec:
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from an external resource with a schema configuration path
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from an external resource without a schema configuration path
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource with a schema configuration path
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:45:46 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource without a schema configuration path
[info] - Load schema from config with a schema configuration path
[info] - Load schema from config without a schema configuration path
[info] - Fail to load schema from config without a schema configuration path
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-04 19:45:46 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:341)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-04 19:45:46 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-04 19:45:46 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:224)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:213)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:341)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:341)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:251)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Failure(org.tupol.spark.io.pureconf.errors$ConfigError: Cannot convert configuration to a scala.runtime.Nothing$. Failures are:
  at the root:
    - (String: 1) Cannot load 'this path does not actually exist': Unable to find 'this path does not actually exist' in the classpath.
    - (String: 1) Cannot convert '{"path":"this path does not actually exist"}' to StructType: Failed to convert the JSON string '{"path":"this path does not actually exist"}' to a data type..
)
[info] - Fail to load schema from a classpath resource
[info] FormatTypeSpec:
[info] - FormatTypeExtractor - custom
[info] - FormatTypeExtractor - avro
[info] - FormatTypeExtractor - xml
[info] - FormatTypeExtractor - xml compact
[info] GenericStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSinkConfiguration out of an empty string
[info] ScalaTest
[info] Run completed in 3 minutes, 38 seconds.
[info] Total number of tests run: 49
[info] Suites: completed 10, aborted 0
[info] Tests: succeeded 49, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 55, Failed 0, Errors 0, Passed 55
[info] ScalaTest
[info] Run completed in 3 minutes, 20 seconds.
[info] Total number of tests run: 96
[info] Suites: completed 27, aborted 0
[info] Tests: succeeded 96, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 96, Failed 0, Errors 0, Passed 96
[info] ScalaTest
[info] Run completed in 3 minutes, 3 seconds.
[info] Total number of tests run: 129
[info] Suites: completed 29, aborted 0
[info] Tests: succeeded 129, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 129, Failed 0, Errors 0, Passed 129
[success] Total time: 269 s (04:29), completed 4 Apr 2024, 19:45:46
[info] Forcing Scala version to 2.13.13 on all projects.
[info] Reapplying settings...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[info] Compiling 15 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/classes ...
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/main/scala/org/tupol/spark/SparkApp.scala:64:38: method copyArrayToImmutableIndexedSeq in class LowPriorityImplicits2 is deprecated (since 2.13.0): implicit conversions from Array to immutable.IndexedSeq are implemented by copying; use `toIndexedSeq` explicitly if you want to copy, or use the more efficient non-copying ArraySeq.unsafeWrapArray
[warn]       config    <- loadConfiguration(args, configurationFileName)
[warn]                                      ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/main/scala/org/tupol/spark/implicits/dataset.scala:78:40: type TraversableOnce in package scala is deprecated (since 2.13.0): Use IterableOnce instead of TraversableOnce
[warn]     def flatMapValues[U: Encoder](f: V => TraversableOnce[U]): Dataset[(K, U)] = {
[warn]                                        ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/main/scala/org/tupol/spark/implicits/dataset.scala:80:45: method map in class IterableOnceExtensionMethods is deprecated (since 2.13.0): Use .iterator.map instead or consider requiring an Iterable
[warn]       dataset.flatMap { case (k, v) => f(v).map((k, _)) }
[warn]                                             ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/main/scala/org/tupol/spark/sql.scala:80:27: method copyArrayToImmutableIndexedSeq in class LowPriorityImplicits2 is deprecated (since 2.13.0): implicit conversions from Array to immutable.IndexedSeq are implemented by copying; use `toIndexedSeq` explicitly if you want to copy, or use the more efficient non-copying ArraySeq.unsafeWrapArray
[warn]           children.flatMap(child => createAliases(child, ancestors :+ field.name))
[warn]                           ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/main/scala/org/tupol/spark/implicits/dataset.scala:38:40: Passing an explicit array value to a Scala varargs method is deprecated (since 2.13.0) and will result in a defensive copy; Use the more efficient non-copying ArraySeq.unsafeWrapArray or an explicit toIndexedSeq call
[warn]         else struct(dataset.columns.map(col): _*)
[warn]                                        ^
[warn] 6 warnings found
[info] Done compiling.
[info] Compiling 20 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.13/classes ...
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/test-classes ...
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/scala/org/tupol/spark/implicits/KeyValueDatasetOpsTest.scala:17:51: method mapValues in trait MapOps is deprecated (since 2.13.0): Use .view.mapValues(f). A future version will include a strict version of this method (for now, .view.mapValues(f).toMap).
[warn]       val expected                        = input.mapValues(_ * 10)
[warn]                                                   ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/scala/org/tupol/spark/testing/package.scala:77:9: method copyArrayToImmutableIndexedSeq in class LowPriorityImplicits2 is deprecated (since 2.13.0): implicit conversions from Array to immutable.IndexedSeq are implemented by copying; use `toIndexedSeq` explicitly if you want to copy, or use the more efficient non-copying ArraySeq.unsafeWrapArray
[warn]         onlyLeftCols,
[warn]         ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/scala/org/tupol/spark/testing/package.scala:78:9: method copyArrayToImmutableIndexedSeq in class LowPriorityImplicits2 is deprecated (since 2.13.0): implicit conversions from Array to immutable.IndexedSeq are implemented by copying; use `toIndexedSeq` explicitly if you want to copy, or use the more efficient non-copying ArraySeq.unsafeWrapArray
[warn]         onlyRightCols,
[warn]         ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/scala/org/tupol/spark/testing/package.scala:60:34: Passing an explicit array value to a Scala varargs method is deprecated (since 2.13.0) and will result in a defensive copy; Use the more efficient non-copying ArraySeq.unsafeWrapArray or an explicit toIndexedSeq call
[warn]           dataOnlyInLeft.orderBy(cols: _*).show(false)
[warn]                                  ^
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/scala/org/tupol/spark/testing/package.scala:64:35: Passing an explicit array value to a Scala varargs method is deprecated (since 2.13.0) and will result in a defensive copy; Use the more efficient non-copying ArraySeq.unsafeWrapArray or an explicit toIndexedSeq call
[warn]           dataOnlyInRight.orderBy(cols: _*).show(false)
[warn]                                   ^
[warn] one warning found
[info] Done compiling.
[info] Compiling 7 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.13/classes ...
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[warn] 6 warnings found
[info] Done compiling.
[info] Compiling 27 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io/target/scala-2.13/test-classes ...
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should not change the name if the name is Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should change the name if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce shorter names if the name is not Avro compliant and replaceWith, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should produce same size names if the name is not Avro compliant and replaceWith is a single char, prefix and suffix are empty: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should prepend the specified prefix if the name is not Avro compliant: OK, passed 100 tests.
[info] + makeNameAvroCompliant.makeNameAvroCompliant should append the specified suffix if the name is not Avro compliant: OK, passed 100 tests.
[info] ConfigSpec:
[info] FuzzyTypesafeConfigBuilder.getConfiguration
[warn] /Users/olivertupran/work/tupol/spark-utils/utils-io/src/test/scala/org/tupol/spark/io/streaming/structured/StreamingSinkConfigurationSpec.scala:30:61: Auto-application to `()` is deprecated. Supply the empty argument list `()` explicitly to invoke method options,
[warn] or remove the empty argument list from its definition (Java-defined methods are exempt).
[warn] In Scala 3, an unapplied method like this will be eta-expanded into a function. [quickfixable]
[warn]       val resultedOptions = tested.addOptions(extraOptions).options
[warn]                                                             ^
2024-04-04 19:46:19 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.whoami="app.param"
MockApp.param="param"
2024-04-04 19:46:19 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-39f8f13d-ec72-41bd-8368-35600745035d/app.conf is available.
2024-04-04 19:46:19 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-39f8f13d-ec72-41bd-8368-35600745035d/app.conf'
2024-04-04 19:46:19 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-04 19:46:19 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
2024-04-04 19:46:19 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-04 19:46:19 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app params then defaults to the Spark app.conf file, then to the app.conf in the classpath and then to reference.conf
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockFun.param="param"
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-b7a17cf5-b47b-4540-9d80-d062b7f26dfb/fun.conf is available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-b7a17cf5-b47b-4540-9d80-d062b7f26dfb/fun.conf'
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/fun.conf is not available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'fun.conf'
2024-04-04 19:46:20 WARN  config$FuzzyTypesafeConfigBuilder:165 - Failed to resolve the variables locally.
2024-04-04 19:46:20 WARN  config$FuzzyTypesafeConfigBuilder:167 - Failed to resolve the variables from the arguments.
[info] - should load first the app.conf then defaults to reference.conf
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param="param"
my.var="MYVAR"
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-c38635de-6791-4ade-9f70-9da73cffccac/app.conf is available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:117 - SparkFiles configuration file: successfully parsed at '/private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-c38635de-6791-4ade-9f70-9da73cffccac/app.conf'
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/app.conf is not available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'app.conf'
[info] - should perform variable substitution
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:
MockApp.param=param
MockApp.my.var=MYVAR
MockApp.substitute.my-other-var=MY_OTHER_VAR
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-8e5c39db-bc68-4fcd-b3f6-0fc9dd02e85f/MockApp/app2.conf is not available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/MockApp/app2.conf is not available.
2024-04-04 19:46:20 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'MockApp/app2.conf'
[info] - should perform variable substitution and addition
[info] SimpleTypesafeConfigBuilder
[info] - should loads first the app params then defaults to app.conf file, then to the app.conf in the classpath and then to reference.conf
[info] - should defaults to reference.conf
[info] - should perform variable substitution
[info] SparkFunSpec:
2024-04-04 19:46:21 INFO  SparkFunSpec$MockFun$:62 - Running MockFun
2024-04-04 19:46:21 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:21 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-abac3e25-7eec-4923-be2a-7d737511baab/application.conf is not available.
2024-04-04 19:46:21 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:21 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:21 DEBUG SparkFunSpec$MockFun$:97 - MockFun: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_fun",
    "whoami" : "./src/test/resources/reference.conf"
}

[warn] two warnings found
[info] Done compiling.
2024-04-04 19:46:22 INFO  SparkFunSpec$MockFun$:72 - MockFun: Job successfully completed.
[info] - SparkFun.main successfully completes
2024-04-04 19:46:22 INFO  SparkFunSpec$MockFunNoConfig:62 - Running MockFunNoConfig
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-af53661c-87e2-4ac7-b240-e2a4635a87d0/application.conf is not available.
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:22 ERROR SparkFunSpec$MockFunNoConfig:101 - MockFunNoConfig: Failed to load application configuration from the MockFunNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunNoConfig'
2024-04-04 19:46:22 INFO  SparkFunSpec$MockFunNoConfig:72 - MockFunNoConfig: Job successfully completed.
[info] - SparkFun.main successfully completes with no configuration expected
2024-04-04 19:46:22 INFO  SparkFunSpec$MockFunFailure:62 - Running MockFunFailure
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-c93622cc-1a45-44c5-b9ee-e6b6546f1e39/application.conf is not available.
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:22 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:22 ERROR SparkFunSpec$MockFunFailure:101 - MockFunFailure: Failed to load application configuration from the MockFunFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/test-classes/reference.conf: 1: No configuration setting found for key 'MockFunFailure'
2024-04-04 19:46:22 ERROR SparkFunSpec$MockFunFailure:73 - MockFunFailure: Job failed.
org.tupol.spark.SparkFunSpec$MockApException
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:54)
	at org.tupol.spark.SparkFunSpec$MockFunFailure$.run(SparkFunSpec.scala:53)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkFun.main(SparkFun.scala:20)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$6(SparkFunSpec.scala:31)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkFunSpec.$anonfun$new$5(SparkFunSpec.scala:31)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkFunSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSpec.scala:11)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkFunSpec.runTest(SparkFunSpec.scala:11)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkFun.main fails gracefully if SparkFun.run fails
[info] - SparkFun.appName gets the simple class name
[info] MakeNameAvroCompliantSpec:
[info] - makeNameAvroCompliant removes the non-compliant chars
[info] - makeNameAvroCompliant replaces the first non-compliant char
[info] - makeNameAvroCompliant does not replace the first non-compliant char if a prefix is specified
[info] - makeNameAvroCompliant does not work with empty strings
[info] - makeNameAvroCompliant reports non-compliant prefix
[info] - makeNameAvroCompliant reports non-compliant replaceWith
[info] - makeNameAvroCompliant reports non-compliant replaceWith first char if prefix is not specified
[info] - makeNameAvroCompliant works with non-compliant replaceWith first char if prefix is specified
[info] - makeNameAvroCompliant reports non-compliant suffix
[warn] one warning found
[info] Done compiling.
[info] Compiling 29 Scala sources to /Users/olivertupran/work/tupol/spark-utils/utils-io-pureconfig/target/scala-2.13/test-classes ...
[warn] -target is deprecated: Use -release instead to compile against the correct platform API.
[info] - makeNameAvroCompliant successfully cleans the schema
[info] KeyValueDatasetOpsTest:
[info] mapValues
[info] - should map only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] flatMapValues
[info] - should flatMap only the values and not the keys
[info] - should return an empty dataset for an empty dataset
[info] SchemaOpsSpec:
[info] - mapFields & checkAllFields
[info] - checkAllFields should fail inside array
[info] - checkAllFields should fail inside map
[info] - checkAnyFields plainly
[info] - checkAnyFields inside arrays
[info] - checkAnyFields inside maps
[info] DataFrameOpsSpec:
[info] - flattenFields on a flat DataFrame should produce no changes.
[info] - flattenFields on a structured DataFrame should flatten it.
2024-04-04 19:46:31 ERROR package:93 - Failed to load text resource from ''.
java.lang.IllegalArgumentException: Cannot load a text resource from an empty path.
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:47)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.utils.UtilsSpec.$anonfun$new$1(UtilsSpec.scala:14)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from local file '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from URI '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from URL '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from classpath '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
2024-04-04 19:46:31 ERROR package:93 - Failed to load text resource from '/unknown/path/leading/to/unknown/file/s1n.ci7y'.
java.lang.IllegalArgumentException: Unable to find '/unknown/path/leading/to/unknown/file/s1n.ci7y' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:230)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.utils.UtilsSpec.$anonfun$new$2(UtilsSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from local file 'utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from URI 'utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from URL 'utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Try loading text resource from classpath 'utils/sample-text.resource'.
2024-04-04 19:46:31 DEBUG package:56 - Successfully loaded text resource from classpath 'utils/sample-text.resource'.
2024-04-04 19:46:32 DEBUG package:56 - Try loading text resource from local file 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:46:32 DEBUG package:56 - Try loading text resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:46:32 DEBUG package:56 - Successfully loaded resource from URI 'file:/Users/olivertupran/work/tupol/spark-utils/utils-core/src/test/resources/utils/sample-text.resource'.
2024-04-04 19:46:32 DEBUG package:56 - Try loading text resource from local file 'http://info.cern.ch'.
2024-04-04 19:46:32 DEBUG package:56 - Try loading text resource from URI 'http://info.cern.ch'.
2024-04-04 19:46:32 DEBUG package:56 - Try loading text resource from URL 'http://info.cern.ch'.
[info] UtilsSpec:
[info] - fuzzyLoadTextResourceFile fails while trying to load an empty path
[info] - fuzzyLoadTextResourceFile fails while trying to load a path that does not exist anywhere
[info] - fuzzyLoadTextResourceFile successfully loads a text from a local path
[info] - fuzzyLoadTextResourceFile successfully loads a text from the class path
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URI
2024-04-04 19:46:32 DEBUG package:56 - Successfully loaded resource from URL 'http://info.cern.ch'.
[info] - fuzzyLoadTextResourceFile successfully loads a text from the URL
[info] MapOpsRowOpsSpec:
[info] - Converting a map to a row and the conversion back from a row to a map
[info] DatasetOpsTest:
[info] withTupledColumn
[info] - should return a tuple of input and column with a simple dataset of 1 value
[info] - should return a tuple of input and column with a simple dataset of 2 values
[info] - should return a tuple of input and column with a simple dataset of nested values
[info] - should return an empty dataset for an empty dataset
[info] - should fail if the specified column type does not match the actual column type
[info] SparkAppSpec:
2024-04-04 19:46:34 INFO  SparkAppSpec$MockApp$:62 - Running MockApp
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-1b0ebf00-99fd-410c-aa31-aeef5568dead/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:34 DEBUG SparkAppSpec$MockApp$:97 - MockApp: Configuration:
{
    "classpath" : {
        "application" : {
            "conf" : false
        }
    },
    "reference" : "reference_mock_app",
    "whoami" : "./src/test/resources/reference.conf"
}

2024-04-04 19:46:34 INFO  SparkAppSpec$MockApp$:72 - MockApp: Job successfully completed.
[info] - SparkApp.main successfully completes
2024-04-04 19:46:34 INFO  SparkAppSpec$MockAppNoConfig:62 - Running MockAppNoConfig
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-5faf1c68-c54a-48a4-ae45-7e6e7af65ba5/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:34 ERROR SparkAppSpec$MockAppNoConfig:101 - MockAppNoConfig: Failed to load application configuration from the MockAppNoConfig path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppNoConfig'
2024-04-04 19:46:34 INFO  SparkAppSpec$MockAppNoConfig:72 - MockAppNoConfig: Job successfully completed.
[info] - SparkApp.main successfully completes with no configuration expected
2024-04-04 19:46:34 INFO  SparkAppSpec$MockAppFailure:62 - Running MockAppFailure
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:94 - Arguments:

2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - SparkFiles configuration file: /private/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark-abe0f379-608b-4a27-9b83-cf56563ea582/userFiles-9548b74d-a7b4-4377-b5eb-b81e5d2583d6/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:113 - Local configuration file: /Users/olivertupran/work/tupol/spark-utils/utils-core/application.conf is not available.
2024-04-04 19:46:34 INFO  config$FuzzyTypesafeConfigBuilder:140 - Successfully parsed the classpath configuration at 'application.conf'
2024-04-04 19:46:34 ERROR SparkAppSpec$MockAppFailure:101 - MockAppFailure: Failed to load application configuration from the MockAppFailure path; using the root configuration instead; merge of system properties,reference.conf @ file:/Users/olivertupran/work/tupol/spark-utils/utils-core/target/scala-2.13/test-classes/reference.conf: 1: No configuration setting found for key 'MockAppFailure'
2024-04-04 19:46:34 ERROR SparkAppSpec$MockAppFailure:73 - MockAppFailure: Job failed.
org.tupol.spark.SparkAppSpec$MockApException
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:56)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.run(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkApp.$anonfun$main$4(SparkApp.scala:68)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.$anonfun$main$2(SparkApp.scala:66)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.$anonfun$main$1(SparkApp.scala:65)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.SparkApp.main(SparkApp.scala:64)
	at org.tupol.spark.SparkApp.main$(SparkApp.scala:61)
	at org.tupol.spark.SparkAppSpec$MockAppFailure$.main(SparkAppSpec.scala:54)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$6(SparkAppSpec.scala:33)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.SparkAppSpec.$anonfun$new$5(SparkAppSpec.scala:33)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.SparkAppSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkAppSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.SparkAppSpec.runTest(SparkAppSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[info] - SparkApp.main fails gracefully if SparkApp.run fails
[info] - SparkApp.appName gets the simple class name
[info] GenericDataSourceSpec:
[warn] one warning found
[info] Done compiling.
2024-04-04 19:46:39 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-04 19:46:39 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:46:39 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$2(GenericDataSourceSpec.scala:23)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2024-04-04 19:46:39 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'.
2024-04-04 19:46:39 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:46:39 ERROR GenericDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'unknown/path/to/inexistent/file.no.way' }, schema: not specified'
	at org.tupol.spark.io.GenericDataSource.$anonfun$read$3(GenericDataSource.scala:67)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSource.read(GenericDataSource.scala:67)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$3(GenericDataSourceSpec.scala:25)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.GenericDataSourceSpec.$anonfun$new$1(GenericDataSourceSpec.scala:25)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.wordspec.AnyWordSpecLike$$anon$3.apply(AnyWordSpecLike.scala:1076)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.wordspec.AnyWordSpec.withFixture(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.invokeWithFixture$1(AnyWordSpecLike.scala:1074)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTest$1(AnyWordSpecLike.scala:1086)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest(AnyWordSpecLike.scala:1086)
	at org.scalatest.wordspec.AnyWordSpecLike.runTest$(AnyWordSpecLike.scala:1068)
	at org.scalatest.wordspec.AnyWordSpec.runTest(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$runTests$1(AnyWordSpecLike.scala:1145)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests(AnyWordSpecLike.scala:1145)
	at org.scalatest.wordspec.AnyWordSpecLike.runTests$(AnyWordSpecLike.scala:1144)
	at org.scalatest.wordspec.AnyWordSpec.runTests(AnyWordSpec.scala:1879)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.wordspec.AnyWordSpec.org$scalatest$wordspec$AnyWordSpecLike$$super$run(AnyWordSpec.scala:1879)
	at org.scalatest.wordspec.AnyWordSpecLike.$anonfun$run$1(AnyWordSpecLike.scala:1190)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.wordspec.AnyWordSpecLike.run(AnyWordSpecLike.scala:1190)
	at org.scalatest.wordspec.AnyWordSpecLike.run$(AnyWordSpecLike.scala:1188)
	at org.tupol.spark.io.GenericDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSourceSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSourceSpec.run(GenericDataSourceSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
[info] - Loading the data fails if the file does not exist
2024-04-04 19:46:39 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-04 19:46:39 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:46:42 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
2024-04-04 19:46:44 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:46:44 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:46:44 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified'.
2024-04-04 19:46:44 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:46:44 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: not specified
[info] - The number of records in the file provided and the schema must match
2024-04-04 19:46:49 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:46:49 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:46:49 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-04 19:46:49 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:46:49 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
2024-04-04 19:46:50 INFO  GenericDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}'.
2024-04-04 19:46:50 DEBUG GenericDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:46:50 INFO  GenericDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'format: 'com.databricks.spark.avro', options: { path: 'src/test/resources/sources/avro/sample.avro' }, schema: {
  "type" : "struct",
  "fields" : [ {
    "name" : "array",
    "type" : {
      "type" : "array",
      "elementType" : "long",
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "dict",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "extra_key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "key",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "int",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "other_string",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
[info] - The number of records in the file provided and the other schema must match
[info] addOptions
[info] - should leave the empty options unchanged
[info] - should leave the options unchanged
[info] - should add extra options
[info] - should override options
[info] withSchema
[info] - should set a new schema
[info] - should change an existing schema
[info] - should remove an existing schema
[info] OrcFileDataSourceSpec:
2024-04-04 19:46:53 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-04 19:46:53 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-04 19:46:53 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
2024-04-04 19:46:54 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-04 19:46:54 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/orc/sample_schema.json'.
2024-04-04 19:46:54 INFO  FileDataSource:53 - Reading data as 'orc' from 'src/test/resources/sources/orc/sample.orc'.
2024-04-04 19:46:54 DEBUG FileDataSource:56 - Initializing the 'orc' DataFrame loader inferring the schema.
2024-04-04 19:46:54 INFO  FileDataSource:53 - Successfully read the data as 'orc' from 'src/test/resources/sources/orc/sample.orc'
[info] - The number of records in the file provided and the schema must match
[info] StreamingSinkConfigurationSpec:
[info] addOptions
[info] - should overwrite existing options for GenericStreamDataSinkConfiguration
[info] - should overwrite existing options for FileStreamDataSinkConfiguration
[info] - should overwrite existing options for KafkaStreamDataSinkConfiguration
[info] FileDataSinkSpec:
2024-04-04 19:46:59 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a768ef70-3bd7-4ae5-ac2a-e181da4dbbfe.temp'.
2024-04-04 19:46:59 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a768ef70-3bd7-4ae5-ac2a-e181da4dbbfe.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a768ef70-3bd7-4ae5-ac2a-e181da4dbbfe.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
[info] - Saving the input data results in the same data
2024-04-04 19:47:03 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp'.
2024-04-04 19:47:03 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
2024-04-04 19:47:03 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp'.
2024-04-04 19:47:03 ERROR FileDataSink:93 - Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp', format: 'parquet', save mode: 'default', partitioning: None, bucketing: None, options: {}).
	at org.tupol.spark.io.FileDataSink.$anonfun$write$12(FileDataSink.scala:115)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:112)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.FileDataAwareSink.write(FileDataSink.scala:122)
	at org.tupol.spark.io.FileDataSinkSpec.$anonfun$new$2(FileDataSinkSpec.scala:37)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.FileDataSinkSpec.runTest(FileDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSinkSpec.run(FileDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7faa00a3-17c9-418c-b988-809c2e835353.temp already exists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1142)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$8(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.Option.getOrElse(Option.scala:201)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$5(FileDataSink.scala:100)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.spark.io.FileDataSink.$anonfun$write$1(FileDataSink.scala:100)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.io.FileDataSink.write(FileDataSink.scala:86)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-04 19:47:03 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-04 19:47:03 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_73e21a4a-a8c2-40aa-a423-66eb232f450d.temp'.
2024-04-04 19:47:04 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_73e21a4a-a8c2-40aa-a423-66eb232f450d.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_73e21a4a-a8c2-40aa-a423-66eb232f450d.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: 'Unchanged', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned results in the same data
2024-04-04 19:47:09 DEBUG FileDataSink:56 - Initializing the DataFrameWriter after repartitioning data to 1 partitions.
2024-04-04 19:47:09 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to partition the data using the following partition columns: [int, string].
2024-04-04 19:47:09 INFO  FileDataSink:53 - Writing data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_dbdff6f9-e078-43e7-b1b1-888fcadba4d2.temp'.
2024-04-04 19:47:10 INFO  FileDataSink:53 - Successfully saved the data as 'parquet' to '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_dbdff6f9-e078-43e7-b1b1-888fcadba4d2.temp' (Full configuration: path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_dbdff6f9-e078-43e7-b1b1-888fcadba4d2.temp', format: 'parquet', save mode: 'default', partitioning: number of partition: '1', partition columns: [int, string], bucketing: None, options: {}).
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-04 19:47:13 DEBUG FileDataSink:56 - Initializing the DataFrameWriter to bucket the data into 1 buckets using the following partition columns: int, string].
2024-04-04 19:47:13 DEBUG FileDataSink:56 - Buckets will be sorted by the following columns: int, string].
2024-04-04 19:47:13 INFO  FileDataSink:53 - Writing data to Hive as 'json' in the 'test_output_table' table due to the buckets configuration: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string]. Notice that the path parameter is used as a table name in this case.
2024-04-04 19:47:13 INFO  FileDataSink:53 - Successfully saved the data as 'json' to 'test_output_table' (Full configuration: path: 'test_output_table', format: 'json', save mode: 'overwrite', partitioning: None, bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: {}).
[info] - Saving the input bucketed results in the same data
[info] FileStreamDataSourceSpec:
2024-04-04 19:47:16 INFO  FileStreamDataSource:53 - Reading data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6e446f36-7e89-4212-acbb-b80f3f8d430a.temp'.
2024-04-04 19:47:16 DEBUG FileStreamDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:47:17 INFO  FileStreamDataSource:53 - Successfully read the data as 'text' from '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_6e446f36-7e89-4212-acbb-b80f3f8d430a.temp'
[info] - String messages should be written to the file stream and read back
[info] - Fail gracefully
[info] GenericFileStreamDataSourceSpec:
2024-04-04 19:47:22 INFO  GenericStreamDataSource:53 - Reading data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7f9833f2-6c5f-4013-8c54-37f1de57f8ac.temp' }, schema: not specified'.
2024-04-04 19:47:22 INFO  GenericStreamDataSource:53 - Successfully read the data as 'text' from 'format: 'text', options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_7f9833f2-6c5f-4013-8c54-37f1de57f8ac.temp' }, schema: not specified'
[info] - String messages should be written to the file stream and read back using a GenericStreamDataSource
[info] - Fail gracefully
[info] FormatTypeSpec:
[info] - fromString works on known types
[info] - fromString returns a Custom format type
[info] CsvFileDataSourceSpec:
2024-04-04 19:47:25 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:25 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:47:25 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-04 19:47:27 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:27 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:47:28 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - The number of records in the csv provided must be the same in the output result
2024-04-04 19:47:33 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'.
2024-04-04 19:47:33 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader inferring the schema.
2024-04-04 19:47:33 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars*.csv'
[info] - The number of records in multiple csv files provided must be the same in the output result
2024-04-04 19:47:35 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:35 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:35 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - User must be able to specify a schema 
2024-04-04 19:47:35 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:35 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:35 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-04 19:47:36 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:36 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:36 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a good csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-04 19:47:37 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'.
2024-04-04 19:47:37 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:38 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars_modified.csv'
[info] - Providing a good csv schema fitting perfectly the data with the parsing option FAILFAST, expecting a success run and a successful dataframe materialization
2024-04-04 19:47:38 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:38 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:38 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option PERMISSIVE, expecting a success run and a successful dataframe materialization
2024-04-04 19:47:41 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:41 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:41 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
[info] - Providing a wrong csv schema with the parsing option DROPMALFORMED, expecting a success run and a successful dataframe materialization
2024-04-04 19:47:42 INFO  FileDataSource:53 - Reading data as 'csv' from 'src/test/resources/sources/csv/cars.csv'.
2024-04-04 19:47:42 DEBUG FileDataSource:56 - Initializing the 'csv' DataFrame loader using the specified schema.
2024-04-04 19:47:42 INFO  FileDataSource:53 - Successfully read the data as 'csv' from 'src/test/resources/sources/csv/cars.csv'
2024-04-04 19:47:43 ERROR Executor:94 - Exception in task 0.0 in stage 59.0 (TID 43)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1236)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:410)
	at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:319)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:264)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:406)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Malformed CSV record
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1037)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:287)
	... 25 more
2024-04-04 19:47:43 ERROR TaskSetManager:73 - Task 0 in stage 59.0 failed 1 times; aborting job
[info] - Providing a wrong csv schema with the parsing option FAILFAST, expecting a success run and a failed dataframe materialization
[info] GenericFileStreamDataSinkSpec:
2024-04-04 19:47:43 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_f3907e51-3eb1-4d41-90e4-bbe63d696703.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_00194b8a-f793-4811-8707-bc1e14c72c2c.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:47:43 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_f3907e51-3eb1-4d41-90e4-bbe63d696703.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_00194b8a-f793-4811-8707-bc1e14c72c2c.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:47:49 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_edd8c7b7-7547-4c0a-acc7-064010a50655.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_66254f33-8f06-4492-998e-485904e80e37.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:47:49 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_edd8c7b7-7547-4c0a-acc7-064010a50655.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_66254f33-8f06-4492-998e-485904e80e37.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] GenericSocketStreamDataSourceSpec:
2024-04-04 19:47:54 INFO  GenericStreamDataSource:53 - Reading data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'.
2024-04-04 19:47:54 INFO  GenericStreamDataSource:53 - Successfully read the data as 'socket' from 'format: 'socket', options: { host: 'localhost', port: '9999' }, schema: not specified'
[info] - String messages should be written to the socket stream and read back
[info] XmlFileDataSourceSpec:
2024-04-04 19:47:58 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:47:58 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:47:58 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:47:58 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:47:58 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:47:58 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
2024-04-04 19:47:59 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:47:59 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:47:59 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:47:59 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract the root element of a single file should yield a single result
2024-04-04 19:48:01 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:48:01 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/xml/sample-schema.json'.
2024-04-04 19:48:01 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'.
2024-04-04 19:48:01 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:01 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:48:01 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-*.xml'
[info] - Extract the root element of multiple files should yield as many results as the number of files
2024-04-04 19:48:01 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'.
2024-04-04 19:48:01 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/sample-1.xml'
[info] - Extract elements that do not exist should return an empty result
2024-04-04 19:48:02 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:48:02 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader inferring the schema.
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Infer simple schema
2024-04-04 19:48:02 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:48:02 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:02 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-04 19:48:02 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:48:02 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-04 19:48:02 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
2024-04-04 19:48:02 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:48:02 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:02 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
2024-04-04 19:48:02 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'.
2024-04-04 19:48:02 DEBUG package$XmlSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:02 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.xml' DataFrame loader using the specified schema.
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-04 19:48:02 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.xml' from 'src/test/resources/sources/xml/test-corrupt-records.xml'
2024-04-04 19:48:02 ERROR Executor:94 - Exception in task 0.0 in stage 54.0 (TID 40)
java.lang.IllegalArgumentException: Malformed line in FAILFAST mode
	at com.databricks.spark.xml.parsers.StaxXmlParser$.failedRecord(StaxXmlParser.scala:106)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:88)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.$anonfun$parse$3(StaxXmlParser.scala:49)
	at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.NumberFormatException: For input string: "abc"
	at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)
	at java.base/java.lang.Long.parseLong(Long.java:711)
	at java.base/java.lang.Long.parseLong(Long.java:836)
	at scala.collection.StringOps$.toLong$extension(StringOps.scala:923)
	at com.databricks.spark.xml.util.TypeCast$.castTo(TypeCast.scala:56)
	at com.databricks.spark.xml.util.TypeCast$.signSafeToLong(TypeCast.scala:277)
	at com.databricks.spark.xml.util.TypeCast$.convertTo(TypeCast.scala:183)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertField(StaxXmlParser.scala:192)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.convertObject(StaxXmlParser.scala:334)
	at com.databricks.spark.xml.parsers.StaxXmlParser$.doParseColumn(StaxXmlParser.scala:82)
	... 21 more
2024-04-04 19:48:02 ERROR TaskSetManager:73 - Task 0 in stage 54.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] FileDataSourceSpec:
2024-04-04 19:48:03 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-04 19:48:03 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:48:03 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$2(FileDataSourceSpec.scala:19)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:19)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2024-04-04 19:48:03 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'.
2024-04-04 19:48:03 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:48:03 ERROR FileDataSource:93 - Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
org.tupol.spark.io.DataSourceException: Failed to read the data as 'com.databricks.spark.avro' from 'unknown/path/to/inexistent/file.no.way'
	at org.tupol.spark.io.FileDataSource.$anonfun$read$5(FileDataSource.scala:66)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.FileDataSource.read(FileDataSource.scala:64)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$3(FileDataSourceSpec.scala:21)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.FileDataSourceSpec.$anonfun$new$1(FileDataSourceSpec.scala:21)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.FileDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(FileDataSourceSpec.scala:10)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.FileDataSourceSpec.run(FileDataSourceSpec.scala:10)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Path does not exist: file:/Users/olivertupran/work/tupol/spark-utils/utils-io/unknown/path/to/inexistent/file.no.way
	at org.apache.spark.sql.errors.QueryCompilationErrors$.dataPathNotExistError(QueryCompilationErrors.scala:978)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:780)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:777)
	at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:687)
	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
	at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
2024-04-04 19:48:03 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-04 19:48:03 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema-2.json'.
2024-04-04 19:48:03 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
[info] - Loading the data fails if the file does not exist
2024-04-04 19:48:03 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:03 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:03 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Loading a json data source works
[info] JdbcDataSinkSpec:
2024-04-04 19:48:03 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data
2024-04-04 19:48:04 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:04 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in the same data with the overwrite save mode
2024-04-04 19:48:05 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:05 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
[info] - Saving the input data results in duplicated data with the append save mode
2024-04-04 19:48:05 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:06 INFO  JdbcDataSink:53 - Writing data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:06 ERROR JdbcDataSink:93 - Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
org.tupol.spark.io.DataSinkException: Failed to save the data as 'jdbc' to the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: url: 'jdbc:h2:~/test', table: 'test_table', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' }).
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$6(JdbcDataSink.scala:65)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:61)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.io.JdbcDataSink.write(JdbcDataSink.scala:50)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.JdbcDataAwareSink.write(JdbcDataSink.scala:74)
	at org.tupol.spark.io.JdbcDataSinkSpec.$anonfun$new$4(JdbcDataSinkSpec.scala:69)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSinkSpec.runTest(JdbcDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSinkSpec.run(JdbcDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: Table or view 'test_table' already exists. SaveMode: ErrorIfExists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.tableOrViewAlreadyExistsError(QueryCompilationErrors.scala:2098)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:72)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$5(JdbcDataSink.scala:59)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.spark.io.JdbcDataSink.$anonfun$write$4(JdbcDataSink.scala:59)
	... 55 more
[info] - Saving the input data over the same table fails with default save mode
[info] GenericKafkaStreamDataSourceSpec:
2024-04-04 19:48:07 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-04 19:48:07 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { kafka.bootstrap.servers: ':6001', subscribe: 'testTopic', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] GenericDataSinkSpec:
2024-04-04 19:48:19 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_e34d7c3a-e047-41d7-931d-48d688d7acc4.temp' }'.
2024-04-04 19:48:19 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_e34d7c3a-e047-41d7-931d-48d688d7acc4.temp' }'.
[info] - Saving the input data results in the same data
2024-04-04 19:48:23 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp' }'.
2024-04-04 19:48:23 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp' }'.
2024-04-04 19:48:23 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp' }'.
2024-04-04 19:48:23 ERROR GenericDataSink:93 - Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp' }').
org.tupol.spark.io.DataSinkException: Failed to save the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp' }').
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$4(GenericDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.GenericDataAwareSink.write(GenericDataSink.scala:66)
	at org.tupol.spark.io.GenericDataSinkSpec.$anonfun$new$2(GenericDataSinkSpec.scala:41)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.GenericDataSinkSpec.runTest(GenericDataSinkSpec.scala:13)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.GenericDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericDataSinkSpec.scala:13)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.GenericDataSinkSpec.run(GenericDataSinkSpec.scala:13)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException: path file:/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_13c64ad2-cf28-4f7b-a7e0-9049879d29ce.temp already exists.
	at org.apache.spark.sql.errors.QueryCompilationErrors$.outputPathAlreadyExistsError(QueryCompilationErrors.scala:1142)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2(GenericDataSink.scala:57)
	at org.tupol.spark.io.GenericDataSink.$anonfun$write$2$adapted(GenericDataSink.scala:57)
	at scala.util.Success.map(Try.scala:270)
	at org.tupol.spark.io.GenericDataSink.write(GenericDataSink.scala:57)
	... 53 more
[info] - Saving the input data can fail if the mode is default and the target file already exists
2024-04-04 19:48:23 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_91754b2e-5287-4893-a491-5bbeba6f586c.temp' }'.
2024-04-04 19:48:24 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_91754b2e-5287-4893-a491-5bbeba6f586c.temp' }'.
[info] - Saving the input partitioned results in the same data
2024-04-04 19:48:25 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3ce4c039-75d8-45a3-82e0-17533ab9e027.temp' }'.
2024-04-04 19:48:25 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [number of partition: 'Unchanged', partition columns: [int, string]], bucketing: None, options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_3ce4c039-75d8-45a3-82e0-17533ab9e027.temp' }'.
[info] - Saving the input partitioned with a partition number specified results in the same data
2024-04-04 19:48:28 INFO  GenericDataSink:53 - Writing data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a19a888c-af32-498e-9379-2a5c0da4ce29.temp' }'.
2024-04-04 19:48:29 INFO  GenericDataSink:53 - Successfully saved the data as 'com.databricks.spark.avro' to 'format: 'com.databricks.spark.avro', save mode: 'default', partitioning: [None], bucketing: number of buckets: '1', bucketing columns: [int, string], sortBy columns: [int, string], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_a19a888c-af32-498e-9379-2a5c0da4ce29.temp' }'.
[info] - Saving the input bucketed results in the same data
[info] StreamingSourceConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileStreamDataSourceConfiguration
[info] - should overwrite existing options for GenericStreamDataSourceConfiguration
[info] - should overwrite existing options for KafkaStreamDataSourceConfiguration
[info] TextFileDataSourceSpec:
2024-04-04 19:48:31 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-04 19:48:31 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:48:31 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
2024-04-04 19:48:31 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/text/sample_schema.json'.
2024-04-04 19:48:31 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/text/sample_schema.json'.
2024-04-04 19:48:31 INFO  FileDataSource:53 - Reading data as 'text' from 'src/test/resources/sources/text/sample.txt'.
2024-04-04 19:48:31 DEBUG FileDataSource:56 - Initializing the 'text' DataFrame loader inferring the schema.
2024-04-04 19:48:31 INFO  FileDataSource:53 - Successfully read the data as 'text' from 'src/test/resources/sources/text/sample.txt'
[info] - The number of records in the file provided and the schema must match
[info] AvroFileDataSourceSpec:
2024-04-04 19:48:32 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:48:32 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:48:33 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-04 19:48:33 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:48:33 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema.json'.
2024-04-04 19:48:33 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:48:33 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader inferring the schema.
2024-04-04 19:48:33 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the schema must match
2024-04-04 19:48:34 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:48:34 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/avro/sample_schema-2.json'.
2024-04-04 19:48:34 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:48:34 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:48:34 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
2024-04-04 19:48:35 INFO  FileDataSource:53 - Reading data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'.
2024-04-04 19:48:35 DEBUG FileDataSource:56 - Initializing the 'com.databricks.spark.avro' DataFrame loader using the specified schema.
2024-04-04 19:48:35 INFO  FileDataSource:53 - Successfully read the data as 'com.databricks.spark.avro' from 'src/test/resources/sources/avro/sample.avro'
[info] - The number of records in the file provided and the other schema must match
[info] GenericStreamDataSinkSpec:
2024-04-04 19:48:38 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_067cc690-d352-4c38-814c-5a3adc91666a.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-04 19:48:38 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_067cc690-d352-4c38-814c-5a3adc91666a.temp' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:48:47 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
2024-04-04 19:48:47 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafkaBootstrapServers: 'unknown_host:0000000', topic: 'testTopic', checkpointLocation: '' }, query name: TestQuery, output mode: not specified, trigger: not specified }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataAwareSink.write(GenericStreamDataSink.scala:66)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$8(GenericStreamDataSinkSpec.scala:72)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$7(GenericStreamDataSinkSpec.scala:72)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningServers(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withTempDir(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningZooKeeper(GenericStreamDataSinkSpec.scala:17)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.withRunningKafka(GenericStreamDataSinkSpec.scala:17)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.$anonfun$new$6(GenericStreamDataSinkSpec.scala:71)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.runTest(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSinkSpec.run(GenericStreamDataSinkSpec.scala:17)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string
	at org.apache.hadoop.fs.Path.checkPathArg(Path.java:172)
	at org.apache.hadoop.fs.Path.<init>(Path.java:184)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.$anonfun$resolveCheckpointLocation$1(ResolveWriteToStream.scala:70)
	at scala.Option.map(Option.scala:242)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:69)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:258)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:423)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:402)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.map(Try.scala:270)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 78 more
[info] - Fail gracefully
[info] JsonFileDataSourceSpec:
2024-04-04 19:48:52 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:48:52 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:48:52 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-04 19:48:52 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:52 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:53 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
2024-04-04 19:48:53 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_1line.json'.
2024-04-04 19:48:53 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:53 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:53 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_1line.json'
[info] - Extract from a single file with a single record should yield a single result
2024-04-04 19:48:56 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:48:56 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'.
2024-04-04 19:48:56 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:56 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/ManyFiles/*.json'
[info] - Extract from multiple files should yield as many results as the total number of records in the files
2024-04-04 19:48:56 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample.json'.
2024-04-04 19:48:56 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader inferring the schema.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample.json'
2024-04-04 19:48:56 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_schema.json'.
2024-04-04 19:48:56 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_schema.json'.
[info] - Infer simple schema
2024-04-04 19:48:56 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:56 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:48:56 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:56 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in default mode (PERMISSIVE)
2024-04-04 19:48:56 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:56 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:48:56 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_customColumnNameOfCorruptRecord' to the input schema.
2024-04-04 19:48:56 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:56 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in PERMISSIVE mode with custom corrupt record column
2024-04-04 19:48:57 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:57 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:57 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:48:57 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:57 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:57 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
[info] - Deal with corrupted records in DROPMALFORMED mode
2024-04-04 19:48:57 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:57 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/json/sample_fail_schema.json'.
2024-04-04 19:48:57 INFO  FileDataSource:53 - Reading data as 'json' from 'src/test/resources/sources/json/sample_fail.json'.
2024-04-04 19:48:57 DEBUG package$JsonSourceConfiguration:56 - The 'columnNameOfCorruptRecord' was specified; adding column '_corrupt_record' to the input schema.
2024-04-04 19:48:57 DEBUG FileDataSource:56 - Initializing the 'json' DataFrame loader using the specified schema.
2024-04-04 19:48:57 INFO  FileDataSource:53 - Successfully read the data as 'json' from 'src/test/resources/sources/json/sample_fail.json'
2024-04-04 19:48:57 ERROR Executor:94 - Exception in task 0.0 in stage 33.0 (TID 23)
org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1236)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)
	at scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:191)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:104)
	at scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:507)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	... 22 more
Caused by: java.lang.RuntimeException: Failed to parse a value for data type bigint (current token: VALUE_NUMBER_FLOAT).
	at org.apache.spark.sql.errors.QueryExecutionErrors$.failToParseValueForDataTypeError(QueryExecutionErrors.scala:974)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:387)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$failedConversion$1.applyOrElse(JacksonParser.scala:370)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$5$1.applyOrElse(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:358)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$5(JacksonParser.scala:189)
	at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:409)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:358)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:482)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2742)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:477)
	... 24 more
2024-04-04 19:48:57 ERROR TaskSetManager:73 - Task 0 in stage 33.0 failed 1 times; aborting job
[info] - Deal with corrupted records in FAILFAST mode
[info] JdbcDataSourceSpec:
2024-04-04 19:48:57 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:57 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
2024-04-04 19:48:57 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:57 INFO  JdbcDataSource:53 - Successfully read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'
[info] - Reading the input data yields the correct result
2024-04-04 19:48:57 INFO  JdbcDataSource:53 - Reading data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test'.
2024-04-04 19:48:57 ERROR JdbcDataSource:93 - Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
org.tupol.spark.io.DataSourceException: Failed to read the data as 'jdbc' from the 'test_table' table of 'jdbc:h2:~/test' (Full configuration: format: 'jdbc', connection properties: { url: 'jdbc:h2:~/test', driver: 'org.h2.Driver', dbtable: 'test_table', user: '', password: '' })
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$3(JdbcDataSource.scala:52)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:49)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$4(JdbcDataSourceSpec.scala:35)
	at org.scalatest.matchers.MatchersHelper$.checkExpectedException(MatchersHelper.scala:298)
	at org.scalatest.matchers.dsl.ResultOfBeWordForAType.thrownBy(ResultOfBeWordForAType.scala:44)
	at org.tupol.spark.io.JdbcDataSourceSpec.$anonfun$new$3(JdbcDataSourceSpec.scala:35)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.JdbcDataSourceSpec.runTest(JdbcDataSourceSpec.scala:12)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.JdbcDataSourceSpec.org$scalatest$BeforeAndAfterAll$$super$run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.JdbcDataSourceSpec.run(JdbcDataSourceSpec.scala:12)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.h2.jdbc.JdbcSQLException: Table "TEST_TABLE" not found; SQL statement:
SELECT * FROM test_table WHERE 1=0 [42102-197]
	at org.h2.message.DbException.getJdbcSQLException(DbException.java:357)
	at org.h2.message.DbException.get(DbException.java:179)
	at org.h2.message.DbException.get(DbException.java:155)
	at org.h2.command.Parser.readTableOrView(Parser.java:5920)
	at org.h2.command.Parser.readTableFilter(Parser.java:1430)
	at org.h2.command.Parser.parseSelectSimpleFromPart(Parser.java:2138)
	at org.h2.command.Parser.parseSelectSimple(Parser.java:2287)
	at org.h2.command.Parser.parseSelectSub(Parser.java:2133)
	at org.h2.command.Parser.parseSelectUnion(Parser.java:1946)
	at org.h2.command.Parser.parseSelect(Parser.java:1919)
	at org.h2.command.Parser.parsePrepared(Parser.java:463)
	at org.h2.command.Parser.parse(Parser.java:335)
	at org.h2.command.Parser.parse(Parser.java:307)
	at org.h2.command.Parser.prepareCommand(Parser.java:278)
	at org.h2.engine.Session.prepareLocal(Session.java:611)
	at org.h2.engine.Session.prepareCommand(Session.java:549)
	at org.h2.jdbc.JdbcConnection.prepareCommand(JdbcConnection.java:1247)
	at org.h2.jdbc.JdbcPreparedStatement.<init>(JdbcPreparedStatement.java:76)
	at org.h2.jdbc.JdbcConnection.prepareStatement(JdbcConnection.java:304)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.getQueryOutputSchema(JDBCRDD.scala:64)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:57)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:239)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:36)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)
	at org.tupol.spark.io.JdbcDataSource.$anonfun$read$2(JdbcDataSource.scala:47)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.spark.io.JdbcDataSource.read(JdbcDataSource.scala:47)
	... 50 more
[info] - Reading the input data fails if table can not be found
[info] SourceConfigurationTest:
[info] schemaWithCorruptRecord
[info] - should return None if there is no schema defined
[info] - should return schema if columnNameOfCorruptRecord is not present in options
2024-04-04 19:48:57 DEBUG SourceConfigurationTest$$anon$3:56 - The 'columnNameOfCorruptRecord' was specified; adding column 'error' to the input schema.
[info] - should return schema with columnNameOfCorruptRecord extra column
[info] addOptions
[info] - should overwrite existing options for CsvSourceConfiguration
[info] - should overwrite existing options for XmlSourceConfiguration
[info] - should overwrite existing options for JsonSourceConfiguration
[info] - should overwrite existing options for ParquetSourceConfiguration
[info] - should overwrite existing options for OrcSourceConfiguration
[info] - should overwrite existing options for AvroSourceConfiguration
[info] - should overwrite existing options for TextSourceConfiguration
[info] - should overwrite existing options for JdbcSourceConfiguration
[info] - should overwrite existing options for GenericSourceConfiguration
[info] KafkaStreamDataSinkSpec:
2024-04-04 19:48:57 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9857465b-a448-47af-a642-c5debc806ff5.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:48:57 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: ':6001', topic: 'testTopic', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_9857465b-a448-47af-a642-c5debc806ff5.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:49:06 INFO  GenericStreamDataSink:53 - Writing data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:49:06 ERROR GenericStreamDataSink:93 - Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
org.tupol.spark.io.DataSinkException: Failed writing the data to { format: 'kafka', partition columns: [], options: { kafka.bootstrap.servers: 'unknown_host:0000000' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$4(GenericStreamDataSink.scala:59)
	at org.tupol.utils.TryOps$TryOps.mapFailure(TryOps.scala:88)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:59)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSink.write(KafkaStreamDataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:43)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:43)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.DataAwareSink.write(DataSink.scala:44)
	at org.tupol.spark.io.DataAwareSink.write$(DataSink.scala:44)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataAwareSink.write(KafkaStreamDataSink.scala:48)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$8(KafkaStreamDataSinkSpec.scala:75)
	at org.scalatest.matchers.should.Matchers.$anonfun$thrownBy$1(Matchers.scala:3029)
	at org.scalatest.matchers.dsl.ResultOfThrownByApplication.execute(ResultOfThrownByApplication.scala:30)
	at org.scalatest.matchers.dsl.ResultOfATypeInvocation.shouldBe(ResultOfATypeInvocation.scala:72)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$7(KafkaStreamDataSinkSpec.scala:75)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$3(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers(EmbeddedKafka.scala:44)
	at io.github.embeddedkafka.EmbeddedKafka.withRunningServers$(EmbeddedKafka.scala:22)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningServers(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$2(EmbeddedKafka.scala:113)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1(EmbeddedKafka.scala:112)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningKafka$1$adapted(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.$anonfun$withRunningZooKeeper$1(EmbeddedKafka.scala:145)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir(EmbeddedKafka.scala:157)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withTempDir$(EmbeddedKafka.scala:152)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withTempDir(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper(EmbeddedKafka.scala:142)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningZooKeeper$(EmbeddedKafka.scala:139)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningZooKeeper(KafkaStreamDataSinkSpec.scala:19)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka(EmbeddedKafka.scala:111)
	at io.github.embeddedkafka.EmbeddedKafkaSupport.withRunningKafka$(EmbeddedKafka.scala:110)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.withRunningKafka(KafkaStreamDataSinkSpec.scala:19)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.$anonfun$new$6(KafkaStreamDataSinkSpec.scala:74)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterEach$$super$runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.runTest(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.org$scalatest$BeforeAndAfterAll$$super$run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.tupol.spark.io.streaming.structured.KafkaStreamDataSinkSpec.run(KafkaStreamDataSinkSpec.scala:19)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.AnalysisException:  checkpointLocation must be specified either through option("checkpointLocation", ...) or SparkSession.conf.set("spark.sql.streaming.checkpointLocation", ...)        
	at org.apache.spark.sql.errors.QueryCompilationErrors$.checkpointLocationNotSpecifiedError(QueryCompilationErrors.scala:2151)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.$anonfun$resolveCheckpointLocation$5(ResolveWriteToStream.scala:85)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:76)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:40)
	at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.apply(ResolveWriteToStream.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:75)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:183)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:183)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:73)
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:258)
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:322)
	at org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:423)
	at org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:402)
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:248)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$3(GenericStreamDataSink.scala:58)
	at scala.util.Success.map(Try.scala:270)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.$anonfun$write$2(GenericStreamDataSink.scala:58)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.spark.io.streaming.structured.GenericStreamDataSink.write(GenericStreamDataSink.scala:58)
	... 79 more
[info] - Fail gracefully
[info] ParquetFileDataSourceSpec:
2024-04-04 19:49:11 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-04 19:49:11 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-04 19:49:12 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
2024-04-04 19:49:12 DEBUG package:56 - Try loading text resource from local file 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-04 19:49:12 DEBUG package:56 - Successfully loaded resource from local path 'src/test/resources/sources/parquet/sample_schema.json'.
2024-04-04 19:49:12 INFO  FileDataSource:53 - Reading data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'.
2024-04-04 19:49:12 DEBUG FileDataSource:56 - Initializing the 'parquet' DataFrame loader inferring the schema.
2024-04-04 19:49:12 INFO  FileDataSource:53 - Successfully read the data as 'parquet' from 'src/test/resources/sources/parquet/sample.parquet'
[info] - The number of records in the file provided and the schema must match
[info] FileStreamDataSinkSpec:
2024-04-04 19:49:13 INFO  GenericStreamDataSink:53 - Writing data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_af307b2c-bcec-4b35-9d31-c46e6bf4029c.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_f59fd501-efb5-46a1-a8c6-bda6c99935d7.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:49:13 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'json', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_af307b2c-bcec-4b35-9d31-c46e6bf4029c.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_f59fd501-efb5-46a1-a8c6-bda6c99935d7.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Json results in the same data
2024-04-04 19:49:14 INFO  GenericStreamDataSink:53 - Writing data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8b8c7dd1-0040-4cb9-bb94-57c3834bd130.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_340613bc-7f4a-472f-a3f4-a21603752f40.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
2024-04-04 19:49:14 INFO  GenericStreamDataSink:53 - Successfully writing the data to { format: 'parquet', partition columns: [], options: { path: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_8b8c7dd1-0040-4cb9-bb94-57c3834bd130.temp', checkpointLocation: '/var/folders/6r/0r60xw6n0c5dhqcrv49cxvbw0000gn/T/spark_tests_340613bc-7f4a-472f-a3f4-a21603752f40.temp' }, query name: testQuery, output mode: not specified, trigger: ProcessingTimeTrigger(1000) }.
[info] - Saving the input data as Parquet results in the same data
[info] KafkaStreamDataSourceSpec:
2024-04-04 19:49:16 INFO  GenericStreamDataSource:53 - Reading data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'.
2024-04-04 19:49:16 INFO  GenericStreamDataSource:53 - Successfully read the data as 'kafka' from 'format: 'kafka', options: { subscribe: 'testTopic', kafka.bootstrap.servers: ':6001', startingOffsets: 'earliest' }, schema: not specified'
[info] - String messages should be written to the kafka stream and read back
[info] - Fail gracefully
[info] SinkConfigurationTest:
[info] addOptions
[info] - should overwrite existing options for FileSinkConfiguration
[info] - should overwrite existing options for GenericSinkConfiguration
[info] OrcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-04 19:49:28 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:28 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:28 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:28 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:28 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSourceConfigurationSpec:
[info] - Successfully extract text FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract json FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract kafka FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract generic FileStreamDataSourceConfiguration out of a configuration string
[info] BucketsConfigurationSpec:
[info] - Successfully extract a full BucketsConfiguration
[info] - Successfully extract a partial BucketsConfiguration
[info] - Failed BucketsConfiguration, missing columns
[info] - Failed BucketsConfiguration, empty columns
[info] - Failed BucketsConfiguration, number = 0
[info] - Failed BucketsConfiguration, number < 0
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JdbcSourceConfigurationSpec:
[info] - Parse configuration without schema
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with path schema
[info] - Parse configuration with explicit schema
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:31 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] XmlSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] GenericStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSourceConfiguration out of an empty string
[info] TextSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] KafkaStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal KafkaStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract KafkaStreamDataSinkConfiguration out of an empty configuration string
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareDataSourceConfigurationSpec:
[info] - Successfully extract FileSourceConfiguration out of a configuration string
[info] - Successfully extract GenericSourceConfiguration out of a configuration string
[info] - Failed to extract FileSourceConfiguration if the path is not defined
[info] - Failed to extract FileSourceConfiguration if the format is not defined
[info] - Failed to extract FileSourceConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string
[info] - Successfully extract JdbcSourceConfiguration out of a configuration string containing only mandatory fields
[info] FileSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create FileSinkConfiguration using the simplified constructor
[info] - Failed to extract FileSinkConfiguration if the path is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Failed to extract FileSinkConfiguration if the format is not acceptable
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] TriggerExtractorSpec:
[info] - TriggerExtractor -> Trigger.Once()
[info] - TriggerExtractor -> Trigger.Continuous() Failure
[info] - TriggerExtractor -> Trigger.Continuous()
[info] - TriggerExtractor -> Trigger.ProcessingTime() Failure
[info] - TriggerExtractor -> Trigger.ProcessingTime()
[info] - TriggerExtractor Fails on unsupported trigger type
[info] - TriggerExtractor Fails on empty
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] JsonSourceConfigurationSpec:
[info] Parse configuration
[info] - should work with schema and no options
[info] - should work without schema
[info] - should work with options
[info] - should fail when path is missing
[info] JdbcSinkConfigurationSpec:
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
[info] - Failed to extract JdbcSinkConfiguration out of an empty configuration string
[info] KafkaStreamDataSourceConfigurationSpec:
[info] - Successfully extract a minimal KafkaStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSourceConfiguration out of a configuration string
[info] - KafkaStreamDataSourceConfiguration required params overwrite the extra options when overlapping
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'kafkaBootstrapServers' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration if 'subscription' is not defined
[info] - Failed to extract KafkaStreamDataSourceConfiguration out of an empty configuration string
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:32 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] FormatAwareStreamingSinkConfigurationSpec:
[info] - Successfully extract a Text FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a Json FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract KafkaStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract GenericStreamDataSinkConfiguration out of a configuration string
[info] - Failed to extract FormatAwareStreamingSinkConfiguration out of a configuration string
[info] PartitionsConfigurationSpec:
[info] - Successfully extract a full PartitionsConfiguration
[info] - Successfully extract a partial PartitionsConfiguration
[info] - Failed PartitionsConfiguration, missing columns
[info] - Failed PartitionsConfiguration, number = 0
[info] - Failed PartitionsConfiguration, number < 0
[info] AvroSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] FileSourceConfigurationSpec:
[info] - Successfully extract a text FileSourceConfiguration out of a configuration string
[info] - Successfully extract a csv FileSourceConfiguration out of a configuration string
[info] ParquetSourceConfigurationSpec:
[info] - Parse configuration without schema
[info] GenericSinkConfigurationSpec:
[info] - Successfully extract GenericSinkConfiguration out of a configuration string
[info] - Successfully extract GenericSinkConfiguration out of a configuration string without the partition files
[info] - Successfully create GenericSinkConfiguration using the simplified constructor
[info] - Successfully extract GenericSinkConfiguration even for a known format
[info] - Failed to extract GenericSinkConfiguration out of an empty configuration string
[info] FileStreamDataSourceConfigurationSpec:
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string
[info] - Successfully extract FileStreamDataSourceConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not supported
[info] - Failed to extract FileStreamDataSourceConfiguration if the path is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is not defined
[info] - Failed to extract FileStreamDataSourceConfiguration if the format is incorrect
[info] - Failed to extract FileStreamDataSourceConfiguration out of an empty configuration string
[info] DataSinkConfigurationSpec:
[info] - Successfully extract FileSinkConfiguration out of a configuration string
[info] - Successfully extract FileSinkConfiguration out of a configuration string without the partition files
[info] - Failed to extract FileSinkConfiguration if the format is not defined
[info] - Successfully extract GenericSinkConfiguration out of a file configuration with a missing path
[info] - Successfully extract GenericSinkConfiguration out of a configuration with an unknown format
[info] - Failed to extract FileSinkConfiguration if the partition.files is a number smaller than 0
[info] - Failed to extract FileSinkConfiguration out of an empty configuration string
[info] - Successfully extract JdbcSinkConfiguration out of a configuration string
[info] - Failed to extract JdbcSinkConfiguration if the url is not defined
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] GenericSourceConfigurationSpec:
[info] - Parse configuration without options
[info] - Parse configuration with options
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Parse configuration with options and schema
[info] KafkaSubscriptionSpec:
[info] - KafkaSubscription.assign
[info] - KafkaSubscription.subscribe
[info] - KafkaSubscription.subscribePattern
[info] - KafkaSubscription Fails on unsupported type
[info] - KafkaSubscription Fails on empty
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] CsvSourceConfigurationSpec:
[info] - Parse configuration with schema
[info] - Parse configuration without schema
[info] - Parse configuration with options
[info] - Parse configuration with options overridden by top level properties
[info] FileStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal FileStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if the format is unsupported
[info] - Failed to extract FileStreamDataSinkConfiguration out of a configuration string if options are missing
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] ExtendedSchemaExtractorSpec:
[info] - Load schema from an external resource with a schema configuration path
[info] - Load schema from an external resource without a schema configuration path
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource with a schema configuration path
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'sources/avro/sample_schema.json'.
2024-04-04 19:49:33 DEBUG package:56 - Successfully loaded text resource from classpath 'sources/avro/sample_schema.json'.
[info] - Load schema from a classpath resource without a schema configuration path
[info] - Load schema from config with a schema configuration path
[info] - Load schema from config without a schema configuration path
[info] - Fail to load schema from config without a schema configuration path
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-04 19:49:33 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:230)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:360)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from local file 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URI 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from URL 'this path does not actually exist'.
2024-04-04 19:49:33 DEBUG package:56 - Try loading text resource from classpath 'this path does not actually exist'.
2024-04-04 19:49:33 ERROR package:93 - Failed to load text resource from 'this path does not actually exist'.
java.lang.IllegalArgumentException: Unable to find 'this path does not actually exist' in the classpath.
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$15(utils.scala:69)
	at scala.util.Failure.orElse(Try.scala:230)
	at org.tupol.spark.utils.package$.createBufferedSource$1(utils.scala:63)
	at org.tupol.spark.utils.package$.$anonfun$fuzzyLoadTextResourceFile$21(utils.scala:76)
	at scala.util.Try$.apply(Try.scala:217)
	at org.tupol.utils.Bracket$.apply(Bracket.scala:43)
	at org.tupol.utils.Bracket$.auto(Bracket.scala:59)
	at org.tupol.spark.utils.package$.fuzzyLoadTextResourceFile(utils.scala:76)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$4(readers.scala:72)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$3(readers.scala:71)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$2(readers.scala:70)
	at scala.util.Either.flatMap(Either.scala:360)
	at org.tupol.spark.io.pureconf.readers$.fromPath$1(readers.scala:69)
	at org.tupol.spark.io.pureconf.readers$.$anonfun$StructTypeReader$1(readers.scala:87)
	at pureconfig.ConfigReader$$anon$1.from(ConfigReader.scala:204)
	at pureconfig.ConfigSource.$anonfun$load$1(ConfigSource.scala:67)
	at scala.util.Either.flatMap(Either.scala:360)
	at pureconfig.ConfigSource.load(ConfigSource.scala:67)
	at pureconfig.ConfigSource.load$(ConfigSource.scala:67)
	at pureconfig.ConfigObjectSource.load(ConfigSource.scala:92)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:71)
	at org.tupol.spark.io.pureconf.config$.$anonfun$extract$3(config.scala:76)
	at scala.util.Success.flatMap(Try.scala:266)
	at org.tupol.spark.io.pureconf.config$.extract(config.scala:76)
	at org.tupol.spark.io.pureconf.config$ConfigOps.extract(config.scala:52)
	at org.tupol.spark.io.pureconf.ExtendedSchemaExtractorSpec.$anonfun$new$8(ExtendedSchemaExtractorSpec.scala:54)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:189)
	at org.scalatest.TestSuite.withFixture(TestSuite.scala:196)
	at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195)
	at org.scalatest.funsuite.AnyFunSuite.withFixture(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:187)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:199)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:199)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:181)
	at org.scalatest.funsuite.AnyFunSuite.runTest(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:232)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:231)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1562)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1562)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:236)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:236)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:235)
	at org.scalatest.funsuite.AnyFunSuite.run(AnyFunSuite.scala:1562)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
	at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:304)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Failure(org.tupol.spark.io.pureconf.errors$ConfigError: Cannot convert configuration to a scala.runtime.Nothing$. Failures are:
  at the root:
    - (String: 1) Cannot load 'this path does not actually exist': Unable to find 'this path does not actually exist' in the classpath.
    - (String: 1) Cannot convert '{"path":"this path does not actually exist"}' to StructType: Failed to convert the JSON string '{"path":"this path does not actually exist"}' to a data type..
)
[info] - Fail to load schema from a classpath resource
[info] FormatTypeSpec:
[info] - FormatTypeExtractor - custom
[info] - FormatTypeExtractor - avro
[info] - FormatTypeExtractor - xml
[info] - FormatTypeExtractor - xml compact
[info] GenericStreamDataSinkConfigurationSpec:
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with empty options
[info] - Successfully extract a minimal GenericStreamDataSinkConfiguration out of a configuration string with options
[info] - Failed to extract GenericStreamDataSinkConfiguration out of an empty string
[info] ScalaTest
[info] Run completed in 3 minutes, 18 seconds.
[info] Total number of tests run: 49
[info] Suites: completed 10, aborted 0
[info] Tests: succeeded 49, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 55, Failed 0, Errors 0, Passed 55
[info] ScalaTest
[info] Run completed in 3 minutes, 11 seconds.
[info] Total number of tests run: 96
[info] Suites: completed 27, aborted 0
[info] Tests: succeeded 96, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 96, Failed 0, Errors 0, Passed 96
[info] ScalaTest
[info] Run completed in 2 minutes, 54 seconds.
[info] Total number of tests run: 129
[info] Suites: completed 29, aborted 0
[info] Tests: succeeded 129, failed 0, canceled 0, ignored 0, pending 0
[info] All tests passed.
[info] Passed: Total 129, Failed 0, Errors 0, Passed 129
[success] Total time: 226 s (03:46), completed 4 Apr 2024, 19:49:33
[info] Reapplying settings...
[info] Set current project to spark-utils (in build file:/Users/olivertupran/work/tupol/spark-utils/)
